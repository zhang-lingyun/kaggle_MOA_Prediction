{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/vikazrajpurohit/3-model-training-and-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    " \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features: (23814, 876)\n",
      "train_targets_scored: (23814, 207)\n",
      "train_targets_nonscored: (23814, 403)\n",
      "train_drug: (23814, 2)\n",
      "test_features: (3982, 876)\n",
      "sample_submission: (3982, 207)\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "data_dir = './data/lish-moa/'\n",
    "\n",
    "train_feat = pd.read_csv(data_dir + 'train_features.csv')\n",
    "scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\n",
    "nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\n",
    "drugs = pd.read_csv(data_dir + 'train_drug.csv')\n",
    "test_feat = pd.read_csv(data_dir + 'test_features.csv')\n",
    "submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "\n",
    "print('train_features: {}'.format(train_feat.shape))\n",
    "print('train_targets_scored: {}'.format(scored.shape))\n",
    "print('train_targets_nonscored: {}'.format(nonscored.shape))\n",
    "print('train_drug: {}'.format(drugs.shape))\n",
    "print('test_features: {}'.format(test_feat.shape))\n",
    "print('sample_submission: {}'.format(submission.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat2=train_feat.copy()\n",
    "test_feat2=test_feat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_feat.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in test_feat.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuantileTransformer\n",
    "将train和test的GENES和CELLS转换为normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# qt for GENES+CELLS\n",
    "qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\n",
    "data = pd.concat([pd.DataFrame(train_feat[GENES+CELLS]), pd.DataFrame(test_feat[GENES+CELLS])])\n",
    "data2 = qt.fit_transform(data[GENES+CELLS])\n",
    "\n",
    "train_feat[GENES+CELLS] = pd.DataFrame(data2[:train_feat.shape[0]])\n",
    "test_feat[GENES+CELLS] = pd.DataFrame(data2[-test_feat.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# seed\n",
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca for GENES\n",
    "n_comp = 600  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_feat[GENES]), pd.DataFrame(test_feat[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_feat.shape[0]]; \n",
    "test2 = data2[-test_feat.shape[0]:]\n",
    "\n",
    "train_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_feat = pd.concat((train_feat, train_gpca), axis=1)\n",
    "test_feat = pd.concat((test_feat, test_gpca), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# pca for CELLS\n",
    "n_comp = 50  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_feat[CELLS]), pd.DataFrame(test_feat[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_feat.shape[0]]\n",
    "test2 = data2[-test_feat.shape[0]:]\n",
    "\n",
    "train_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_feat = pd.concat((train_feat, train_cpca), axis=1)\n",
    "test_feat = pd.concat((test_feat, test_cpca), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 1526), (23814, 1526))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat.shape,train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# var threshold for GENES CELLS features\n",
    "var_thresh = VarianceThreshold(0.85)  #<-- Update\n",
    "\n",
    "data = train_feat.append(test_feat)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_feat.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_feat.shape[0] : ]\n",
    "\n",
    "\n",
    "train_feat = pd.DataFrame(train_feat[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_feat = pd.concat([train_feat, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_feat = pd.DataFrame(test_feat[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_feat = pd.concat([test_feat, pd.DataFrame(test_features_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 1028), (23814, 1028))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat.shape,train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# cluster feature for GENES and CELLS\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def fe_cluster(train, test, n_clusters_g = 22, n_clusters_c = 4, SEED = 42):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "    return train, test\n",
    "\n",
    "train_feat2,test_feat2=fe_cluster(train_feat2,test_feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# cluster for GENES and CELLS PCA features\n",
    "def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n",
    "        data=pd.concat([train,test],axis=0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "        train[f'clusters_pca'] = kmeans.labels_[:train.shape[0]]\n",
    "        test[f'clusters_pca'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n",
    "        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n",
    "        return train, test\n",
    "\n",
    "train_pca=pd.concat((train_gpca,train_cpca),axis=1)\n",
    "test_pca=pd.concat((test_gpca,test_cpca),axis=1)\n",
    "train_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cluster_pca = train_cluster_pca.iloc[:,650:]\n",
    "test_cluster_pca = test_cluster_pca.iloc[:,650:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_cluster=train_feat2.iloc[:,876:]\n",
    "test_features_cluster=test_feat2.iloc[:,876:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n",
    "len(gsquarecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# statistic feature for GENES and CELLS\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "        df['c52_c42'] = df['c-52'] * df['c-42']\n",
    "        df['c13_c73'] = df['c-13'] * df['c-73']\n",
    "        df['c26_c13'] = df['c-23'] * df['c-13']\n",
    "        df['c33_c6'] = df['c-33'] * df['c-6']\n",
    "        df['c11_c55'] = df['c-11'] * df['c-55']\n",
    "        df['c38_c63'] = df['c-38'] * df['c-63']\n",
    "        df['c38_c94'] = df['c-38'] * df['c-94']\n",
    "        df['c13_c94'] = df['c-13'] * df['c-94']\n",
    "        df['c4_c52'] = df['c-4'] * df['c-52']\n",
    "        df['c4_c42'] = df['c-4'] * df['c-42']\n",
    "        df['c13_c38'] = df['c-13'] * df['c-38']\n",
    "        df['c55_c2'] = df['c-55'] * df['c-2']\n",
    "        df['c55_c4'] = df['c-55'] * df['c-4']\n",
    "        df['c4_c13'] = df['c-4'] * df['c-13']\n",
    "        df['c82_c42'] = df['c-82'] * df['c-42']\n",
    "        df['c66_c42'] = df['c-66'] * df['c-42']\n",
    "        df['c6_c38'] = df['c-6'] * df['c-38']\n",
    "        df['c2_c13'] = df['c-2'] * df['c-13']\n",
    "        df['c62_c42'] = df['c-62'] * df['c-42']\n",
    "        df['c90_c55'] = df['c-90'] * df['c-55']\n",
    "        \n",
    "        \n",
    "        for feature in features_c:\n",
    "             df[f'{feature}_squared'] = df[feature] ** 2     \n",
    "                \n",
    "        for feature in gsquarecols:\n",
    "            df[f'{feature}_squared'] = df[feature] ** 2        \n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_feat2,test_feat2=fe_stats(train_feat2,test_feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_stats=train_feat2.iloc[:,902:]\n",
    "test_features_stats=test_feat2.iloc[:,902:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 1028), (3982, 1028))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat.shape,test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 1239), (3982, 1239))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat = pd.concat((train_feat, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\n",
    "test_feat = pd.concat((test_feat, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)\n",
    "\n",
    "train_feat.shape,test_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21948, 1848), (3624, 1239))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train,test\n",
    "train = train_feat.merge(scored, on='sig_id')\n",
    "train = train.merge(nonscored, on='sig_id')\n",
    "train = train.merge(drugs, on='sig_id')\n",
    "train = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_feat[test_feat['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_targets: 206\n",
      "num_aux_targets: 402\n",
      "num_all_targets: 608\n"
     ]
    }
   ],
   "source": [
    "# targets\n",
    "target_cols = [x for x in scored.columns if x != 'sig_id']\n",
    "aux_target_cols = [x for x in nonscored.columns if x != 'sig_id']\n",
    "all_target_cols = target_cols + aux_target_cols\n",
    "\n",
    "num_targets = len(target_cols)\n",
    "num_aux_targets = len(aux_target_cols)\n",
    "num_all_targets = len(all_target_cols)\n",
    "\n",
    "print('num_targets: {}'.format(num_targets))\n",
    "print('num_aux_targets: {}'.format(num_aux_targets))\n",
    "print('num_all_targets: {}'.format(num_all_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1847)\n",
      "(3624, 1238)\n",
      "(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(submission.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Tabnet \n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# SmoothBCEwLogits\n",
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "            \n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1240"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data\n",
    "\n",
    "feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\n",
    "num_features = len(feature_cols)\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>xanthine_oxidase_inhibitor</th>\n",
       "      <th>xiap_inhibitor</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>kfold_0</th>\n",
       "      <th>kfold_1</th>\n",
       "      <th>kfold_2</th>\n",
       "      <th>kfold_3</th>\n",
       "      <th>kfold_4</th>\n",
       "      <th>kfold_5</th>\n",
       "      <th>kfold_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.146806</td>\n",
       "      <td>0.902075</td>\n",
       "      <td>-0.418339</td>\n",
       "      <td>-0.961202</td>\n",
       "      <td>-0.254770</td>\n",
       "      <td>-1.021300</td>\n",
       "      <td>-1.369236</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>b68db1d53</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.128824</td>\n",
       "      <td>0.676862</td>\n",
       "      <td>0.274345</td>\n",
       "      <td>0.090495</td>\n",
       "      <td>1.208863</td>\n",
       "      <td>0.688965</td>\n",
       "      <td>0.316734</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>df89a8e5a</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.790372</td>\n",
       "      <td>0.939951</td>\n",
       "      <td>1.428097</td>\n",
       "      <td>-0.121817</td>\n",
       "      <td>-0.002067</td>\n",
       "      <td>1.495091</td>\n",
       "      <td>0.238763</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18bb41b2c</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.729866</td>\n",
       "      <td>-0.277163</td>\n",
       "      <td>-0.441200</td>\n",
       "      <td>0.766612</td>\n",
       "      <td>2.347817</td>\n",
       "      <td>-0.862761</td>\n",
       "      <td>-2.308829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8c7f86626</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.444558</td>\n",
       "      <td>-0.481202</td>\n",
       "      <td>0.974729</td>\n",
       "      <td>0.977467</td>\n",
       "      <td>1.468304</td>\n",
       "      <td>-0.874772</td>\n",
       "      <td>-0.372682</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7cbed3131</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1854 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id cp_time cp_dose         0         1         2         3  \\\n",
       "0  id_000644bb2      24      D1  1.146806  0.902075 -0.418339 -0.961202   \n",
       "1  id_000779bfc      72      D1  0.128824  0.676862  0.274345  0.090495   \n",
       "2  id_000a6266a      48      D1  0.790372  0.939951  1.428097 -0.121817   \n",
       "3  id_0015fd391      48      D1 -0.729866 -0.277163 -0.441200  0.766612   \n",
       "4  id_001626bd3      72      D2 -0.444558 -0.481202  0.974729  0.977467   \n",
       "\n",
       "          4         5         6  ...  xanthine_oxidase_inhibitor  \\\n",
       "0 -0.254770 -1.021300 -1.369236  ...                           0   \n",
       "1  1.208863  0.688965  0.316734  ...                           0   \n",
       "2 -0.002067  1.495091  0.238763  ...                           0   \n",
       "3  2.347817 -0.862761 -2.308829  ...                           0   \n",
       "4  1.468304 -0.874772 -0.372682  ...                           0   \n",
       "\n",
       "   xiap_inhibitor    drug_id  kfold_0  kfold_1  kfold_2  kfold_3  kfold_4  \\\n",
       "0               0  b68db1d53        1        3        3        5        0   \n",
       "1               0  df89a8e5a        0        3        6        3        3   \n",
       "2               0  18bb41b2c        5        3        3        1        3   \n",
       "3               0  8c7f86626        4        3        1        2        4   \n",
       "4               0  7cbed3131        6        5        3        3        2   \n",
       "\n",
       "   kfold_5  kfold_6  \n",
       "0        5        2  \n",
       "1        4        1  \n",
       "2        0        4  \n",
       "3        5        2  \n",
       "4        1        4  \n",
       "\n",
       "[5 rows x 1854 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv_folds\n",
    "from sklearn.model_selection import KFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "def make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n",
    "    vc = train.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n",
    "    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n",
    "\n",
    "    for seed_id in range(SEEDS):\n",
    "        kfold_col = 'kfold_{}'.format(seed_id)\n",
    "        \n",
    "        # STRATIFY DRUGS 18X OR LESS\n",
    "        dct1 = {}\n",
    "        dct2 = {}\n",
    "\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.index[idxV].values}\n",
    "            dct1.update(dd)\n",
    "\n",
    "        # STRATIFY DRUGS MORE THAN 18X\n",
    "        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n",
    "        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "\n",
    "        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n",
    "            dd = {k: fold for k in tmp.sig_id[idxV].values}\n",
    "            dct2.update(dd)\n",
    "\n",
    "        # ASSIGN FOLDS\n",
    "        train[kfold_col] = train.drug_id.map(dct1)\n",
    "        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n",
    "        train[kfold_col] = train[kfold_col].astype('int8')\n",
    "        \n",
    "    return train\n",
    "\n",
    "SEEDS = 7\n",
    "NFOLDS = 7\n",
    "DRUG_THRESH = 18\n",
    "\n",
    "train = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_training(fold_id, seed_id):\n",
    "    seed_everything(seed_id)\n",
    "    \n",
    "    train_ = process_data(train)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    kfold_col = f'kfold_{seed_id}'\n",
    "    trn_idx = train_[train_[kfold_col] != fold_id].index\n",
    "    val_idx = train_[train_[kfold_col] == fold_id].index\n",
    "    \n",
    "    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n",
    "    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n",
    "    \n",
    "    def train_model(model, tag_name, target_cols_now):\n",
    "        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n",
    "        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n",
    "        \n",
    "        model.fit(\n",
    "            X_train = x_train,\n",
    "            y_train = y_train,\n",
    "            eval_set = [(x_valid, y_valid)],\n",
    "            eval_name = [\"val\"],\n",
    "            eval_metric = [\"logits_ll\"],\n",
    "            max_epochs = MAX_EPOCH,\n",
    "            patience = 20,\n",
    "            batch_size = 1024, \n",
    "            virtual_batch_size = 32,\n",
    "            num_workers = 1,\n",
    "            drop_last = False,\n",
    "            # To use binary cross entropy because this is not a regression problem\n",
    "            loss_fn = SmoothBCEwLogits(smoothing=5e-5) # F.binary_cross_entropy_with_logits\n",
    "        )\n",
    "        # save tabnet model\n",
    "        print('Model Saving:',f\"{tag_name}_FOLD{fold_id}_.pth\")\n",
    "        saving_path_name =  f\"{tag_name}_FOLD{fold_id}_.pth\"\n",
    "        saved_filepath = model.save_model(saving_path_name) # loaded_clf.load_model(saved_filepath)\n",
    "        \n",
    "        preds_val = model.predict(x_valid)\n",
    "        preds = 1 / (1 + np.exp(-preds_val))\n",
    "        \n",
    "        oof = np.zeros((len(train), len(target_cols_now)))\n",
    "        oof[val_idx] = preds\n",
    "        return oof\n",
    "    \n",
    "    tabnet_params = dict(\n",
    "        n_d = 32,\n",
    "        n_a = 32,\n",
    "        n_steps = 1,\n",
    "        gamma = 1.3,\n",
    "        lambda_sparse = 0,\n",
    "        optimizer_fn = optim.Adam,\n",
    "        optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n",
    "        mask_type = \"entmax\",\n",
    "        scheduler_params = dict(mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "        scheduler_fn = ReduceLROnPlateau,\n",
    "        seed = seed_id,\n",
    "        verbose = 10\n",
    "    )\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "    # Fine-tune the model on scored targets only\n",
    "    oof = train_model(model, 'tabnet_v2', target_cols)\n",
    "\n",
    "    #--------------------- PREDICTION---------------------\n",
    "    \n",
    "    x_test = test_[feature_cols].values\n",
    "    preds = model.predict(x_test)\n",
    "    preds = (1 / (1 + np.exp(-preds)))\n",
    "\n",
    "    return oof, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed_id):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold_id in range(NFOLDS):\n",
    "        print(f'Seed:{seed_id},Fold:{fold_id}')\n",
    "        oof_, pred_ = run_training(fold_id, seed_id)\n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:0,Fold:0\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.38    | val_logits_ll: 0.04423 |  0:00:02s\n",
      "epoch 10 | loss: 0.0212  | val_logits_ll: 0.02268 |  0:00:28s\n",
      "epoch 20 | loss: 0.02028 | val_logits_ll: 0.01981 |  0:00:53s\n",
      "epoch 30 | loss: 0.02    | val_logits_ll: 0.01964 |  0:01:18s\n",
      "epoch 40 | loss: 0.01988 | val_logits_ll: 0.0195  |  0:01:43s\n",
      "epoch 50 | loss: 0.01982 | val_logits_ll: 0.01944 |  0:02:10s\n",
      "epoch 60 | loss: 0.01972 | val_logits_ll: 0.01939 |  0:02:35s\n",
      "epoch 70 | loss: 0.01966 | val_logits_ll: 0.01935 |  0:03:01s\n",
      "epoch 80 | loss: 0.01971 | val_logits_ll: 0.01947 |  0:03:27s\n",
      "epoch 90 | loss: 0.01966 | val_logits_ll: 0.01931 |  0:03:52s\n",
      "epoch 100| loss: 0.0196  | val_logits_ll: 0.01927 |  0:04:18s\n",
      "\n",
      "Early stopping occured at epoch 104 with best_epoch = 84 and best_val_logits_ll = 0.01923\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD0_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD0_.pth.zip\n",
      "Seed:0,Fold:1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.38576 | val_logits_ll: 0.04128 |  0:00:02s\n",
      "epoch 10 | loss: 0.02131 | val_logits_ll: 0.0209  |  0:00:28s\n",
      "epoch 20 | loss: 0.02023 | val_logits_ll: 0.02196 |  0:00:54s\n",
      "epoch 30 | loss: 0.02    | val_logits_ll: 0.02112 |  0:01:19s\n",
      "epoch 40 | loss: 0.01988 | val_logits_ll: 0.01957 |  0:01:45s\n",
      "epoch 50 | loss: 0.01977 | val_logits_ll: 0.01941 |  0:02:11s\n",
      "epoch 60 | loss: 0.01973 | val_logits_ll: 0.01935 |  0:02:36s\n",
      "epoch 70 | loss: 0.01971 | val_logits_ll: 0.01937 |  0:03:01s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01922 |  0:03:27s\n",
      "epoch 90 | loss: 0.01961 | val_logits_ll: 0.01925 |  0:03:53s\n",
      "\n",
      "Early stopping occured at epoch 93 with best_epoch = 73 and best_val_logits_ll = 0.01919\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD1_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD1_.pth.zip\n",
      "Seed:0,Fold:2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3796  | val_logits_ll: 0.0405  |  0:00:02s\n",
      "epoch 10 | loss: 0.02147 | val_logits_ll: 0.02215 |  0:00:28s\n",
      "epoch 20 | loss: 0.02034 | val_logits_ll: 0.02129 |  0:00:53s\n",
      "epoch 30 | loss: 0.02001 | val_logits_ll: 0.02003 |  0:01:20s\n",
      "epoch 40 | loss: 0.01996 | val_logits_ll: 0.02052 |  0:01:45s\n",
      "epoch 50 | loss: 0.01975 | val_logits_ll: 0.01964 |  0:02:11s\n",
      "epoch 60 | loss: 0.01977 | val_logits_ll: 0.01959 |  0:02:37s\n",
      "epoch 70 | loss: 0.01969 | val_logits_ll: 0.01952 |  0:03:04s\n",
      "epoch 80 | loss: 0.0197  | val_logits_ll: 0.01953 |  0:03:30s\n",
      "epoch 90 | loss: 0.01962 | val_logits_ll: 0.01943 |  0:03:55s\n",
      "epoch 100| loss: 0.01963 | val_logits_ll: 0.01945 |  0:04:22s\n",
      "epoch 110| loss: 0.01961 | val_logits_ll: 0.01939 |  0:04:48s\n",
      "epoch 120| loss: 0.01956 | val_logits_ll: 0.01943 |  0:05:14s\n",
      "epoch 130| loss: 0.01956 | val_logits_ll: 0.01939 |  0:05:40s\n",
      "epoch 140| loss: 0.01953 | val_logits_ll: 0.01938 |  0:06:05s\n",
      "epoch 150| loss: 0.01953 | val_logits_ll: 0.01939 |  0:06:31s\n",
      "epoch 160| loss: 0.0195  | val_logits_ll: 0.01936 |  0:06:57s\n",
      "epoch 170| loss: 0.01951 | val_logits_ll: 0.01933 |  0:07:23s\n",
      "epoch 180| loss: 0.01949 | val_logits_ll: 0.01932 |  0:07:49s\n",
      "epoch 190| loss: 0.01947 | val_logits_ll: 0.01931 |  0:08:14s\n",
      "\n",
      "Early stopping occured at epoch 199 with best_epoch = 179 and best_val_logits_ll = 0.0193\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD2_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD2_.pth.zip\n",
      "Seed:0,Fold:3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.38099 | val_logits_ll: 0.04105 |  0:00:02s\n",
      "epoch 10 | loss: 0.02146 | val_logits_ll: 0.02318 |  0:00:29s\n",
      "epoch 20 | loss: 0.02008 | val_logits_ll: 0.02301 |  0:00:54s\n",
      "epoch 30 | loss: 0.01997 | val_logits_ll: 0.02144 |  0:01:20s\n",
      "epoch 40 | loss: 0.01978 | val_logits_ll: 0.0196  |  0:01:45s\n",
      "epoch 50 | loss: 0.01969 | val_logits_ll: 0.0195  |  0:02:12s\n",
      "epoch 60 | loss: 0.01965 | val_logits_ll: 0.01948 |  0:02:38s\n",
      "epoch 70 | loss: 0.01965 | val_logits_ll: 0.01956 |  0:03:03s\n",
      "epoch 80 | loss: 0.01959 | val_logits_ll: 0.01946 |  0:03:29s\n",
      "epoch 90 | loss: 0.01961 | val_logits_ll: 0.01942 |  0:03:54s\n",
      "\n",
      "Early stopping occured at epoch 99 with best_epoch = 79 and best_val_logits_ll = 0.01937\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD3_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD3_.pth.zip\n",
      "Seed:0,Fold:4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.38339 | val_logits_ll: 0.04161 |  0:00:02s\n",
      "epoch 10 | loss: 0.02149 | val_logits_ll: 0.02127 |  0:00:29s\n",
      "epoch 20 | loss: 0.02031 | val_logits_ll: 0.02009 |  0:00:55s\n",
      "epoch 30 | loss: 0.02    | val_logits_ll: 0.02266 |  0:01:21s\n",
      "epoch 40 | loss: 0.01986 | val_logits_ll: 0.01969 |  0:01:47s\n",
      "epoch 50 | loss: 0.01981 | val_logits_ll: 0.01959 |  0:02:13s\n",
      "epoch 60 | loss: 0.01967 | val_logits_ll: 0.01946 |  0:02:39s\n",
      "epoch 70 | loss: 0.01969 | val_logits_ll: 0.01943 |  0:03:05s\n",
      "epoch 80 | loss: 0.01968 | val_logits_ll: 0.01943 |  0:03:31s\n",
      "epoch 90 | loss: 0.01959 | val_logits_ll: 0.01937 |  0:03:57s\n",
      "epoch 100| loss: 0.01962 | val_logits_ll: 0.01936 |  0:04:23s\n",
      "epoch 110| loss: 0.01957 | val_logits_ll: 0.01934 |  0:04:49s\n",
      "epoch 120| loss: 0.01955 | val_logits_ll: 0.01935 |  0:05:14s\n",
      "epoch 130| loss: 0.01955 | val_logits_ll: 0.01933 |  0:05:40s\n",
      "epoch 140| loss: 0.01954 | val_logits_ll: 0.01932 |  0:06:06s\n",
      "epoch 150| loss: 0.01952 | val_logits_ll: 0.0193  |  0:06:32s\n",
      "\n",
      "Early stopping occured at epoch 155 with best_epoch = 135 and best_val_logits_ll = 0.01927\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD4_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD4_.pth.zip\n",
      "Seed:0,Fold:5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.38078 | val_logits_ll: 0.04186 |  0:00:02s\n",
      "epoch 10 | loss: 0.02138 | val_logits_ll: 0.0218  |  0:00:27s\n",
      "epoch 20 | loss: 0.02022 | val_logits_ll: 0.02074 |  0:00:54s\n",
      "epoch 30 | loss: 0.01992 | val_logits_ll: 0.02034 |  0:01:19s\n",
      "epoch 40 | loss: 0.01984 | val_logits_ll: 0.01979 |  0:01:45s\n",
      "epoch 50 | loss: 0.01973 | val_logits_ll: 0.01974 |  0:02:09s\n",
      "epoch 60 | loss: 0.01968 | val_logits_ll: 0.01968 |  0:02:35s\n",
      "epoch 70 | loss: 0.01971 | val_logits_ll: 0.01972 |  0:03:01s\n",
      "epoch 80 | loss: 0.01963 | val_logits_ll: 0.01959 |  0:03:26s\n",
      "epoch 90 | loss: 0.01963 | val_logits_ll: 0.01957 |  0:03:51s\n",
      "epoch 100| loss: 0.01958 | val_logits_ll: 0.0195  |  0:04:17s\n",
      "epoch 110| loss: 0.01958 | val_logits_ll: 0.01951 |  0:04:43s\n",
      "epoch 120| loss: 0.01955 | val_logits_ll: 0.01949 |  0:05:10s\n",
      "epoch 130| loss: 0.01956 | val_logits_ll: 0.01952 |  0:05:36s\n",
      "epoch 140| loss: 0.0195  | val_logits_ll: 0.01946 |  0:06:02s\n",
      "epoch 150| loss: 0.01955 | val_logits_ll: 0.01955 |  0:06:28s\n",
      "epoch 160| loss: 0.01948 | val_logits_ll: 0.01947 |  0:06:54s\n",
      "epoch 170| loss: 0.01947 | val_logits_ll: 0.01943 |  0:07:20s\n",
      "epoch 180| loss: 0.01946 | val_logits_ll: 0.01943 |  0:07:46s\n",
      "epoch 190| loss: 0.01944 | val_logits_ll: 0.01947 |  0:08:12s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 192 and best_val_logits_ll = 0.0194\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD5_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD5_.pth.zip\n",
      "Seed:0,Fold:6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.38966 | val_logits_ll: 0.04391 |  0:00:02s\n",
      "epoch 10 | loss: 0.02152 | val_logits_ll: 0.02101 |  0:00:28s\n",
      "epoch 20 | loss: 0.02016 | val_logits_ll: 0.02071 |  0:00:54s\n",
      "epoch 30 | loss: 0.01992 | val_logits_ll: 0.01985 |  0:01:18s\n",
      "epoch 40 | loss: 0.01983 | val_logits_ll: 0.01964 |  0:01:44s\n",
      "epoch 50 | loss: 0.01975 | val_logits_ll: 0.01963 |  0:02:10s\n",
      "epoch 60 | loss: 0.01967 | val_logits_ll: 0.01995 |  0:02:35s\n",
      "epoch 70 | loss: 0.01968 | val_logits_ll: 0.01956 |  0:03:01s\n",
      "epoch 80 | loss: 0.01973 | val_logits_ll: 0.01956 |  0:03:27s\n",
      "epoch 90 | loss: 0.01969 | val_logits_ll: 0.01955 |  0:03:53s\n",
      "epoch 100| loss: 0.01958 | val_logits_ll: 0.01944 |  0:04:19s\n",
      "\n",
      "Early stopping occured at epoch 106 with best_epoch = 86 and best_val_logits_ll = 0.01942\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD6_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD6_.pth.zip\n",
      "Seed:1,Fold:0\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.33953 | val_logits_ll: 0.03493 |  0:00:02s\n",
      "epoch 10 | loss: 0.02105 | val_logits_ll: 0.02251 |  0:00:28s\n",
      "epoch 20 | loss: 0.02031 | val_logits_ll: 0.02236 |  0:00:54s\n",
      "epoch 30 | loss: 0.01994 | val_logits_ll: 0.01986 |  0:01:19s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 | loss: 0.0199  | val_logits_ll: 0.02011 |  0:01:45s\n",
      "epoch 50 | loss: 0.01975 | val_logits_ll: 0.01947 |  0:02:10s\n",
      "epoch 60 | loss: 0.0197  | val_logits_ll: 0.0194  |  0:02:35s\n",
      "epoch 70 | loss: 0.01961 | val_logits_ll: 0.01941 |  0:03:01s\n",
      "epoch 80 | loss: 0.01964 | val_logits_ll: 0.01936 |  0:03:27s\n",
      "epoch 90 | loss: 0.01958 | val_logits_ll: 0.01936 |  0:03:52s\n",
      "epoch 100| loss: 0.01958 | val_logits_ll: 0.01934 |  0:04:18s\n",
      "epoch 110| loss: 0.01964 | val_logits_ll: 0.01936 |  0:04:44s\n",
      "epoch 120| loss: 0.01959 | val_logits_ll: 0.01933 |  0:05:09s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.01931 |  0:05:34s\n",
      "epoch 140| loss: 0.01952 | val_logits_ll: 0.01932 |  0:06:00s\n",
      "epoch 150| loss: 0.01952 | val_logits_ll: 0.0193  |  0:06:27s\n",
      "epoch 160| loss: 0.01951 | val_logits_ll: 0.01929 |  0:06:53s\n",
      "epoch 170| loss: 0.01949 | val_logits_ll: 0.01927 |  0:07:18s\n",
      "epoch 180| loss: 0.01948 | val_logits_ll: 0.01929 |  0:07:43s\n",
      "epoch 190| loss: 0.01947 | val_logits_ll: 0.01928 |  0:08:09s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 197 and best_val_logits_ll = 0.01926\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD0_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD0_.pth.zip\n",
      "Seed:1,Fold:1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.33885 | val_logits_ll: 0.03648 |  0:00:02s\n",
      "epoch 10 | loss: 0.02141 | val_logits_ll: 0.02101 |  0:00:28s\n",
      "epoch 20 | loss: 0.0202  | val_logits_ll: 0.02254 |  0:00:54s\n",
      "epoch 30 | loss: 0.01996 | val_logits_ll: 0.01965 |  0:01:19s\n",
      "epoch 40 | loss: 0.01983 | val_logits_ll: 0.02002 |  0:01:45s\n",
      "epoch 50 | loss: 0.01984 | val_logits_ll: 0.01951 |  0:02:11s\n",
      "epoch 60 | loss: 0.01974 | val_logits_ll: 0.01963 |  0:02:38s\n",
      "epoch 70 | loss: 0.01972 | val_logits_ll: 0.01948 |  0:03:03s\n",
      "epoch 80 | loss: 0.01965 | val_logits_ll: 0.01942 |  0:03:28s\n",
      "epoch 90 | loss: 0.0196  | val_logits_ll: 0.01936 |  0:03:53s\n",
      "epoch 100| loss: 0.01965 | val_logits_ll: 0.01934 |  0:04:18s\n",
      "epoch 110| loss: 0.01963 | val_logits_ll: 0.01938 |  0:04:44s\n",
      "epoch 120| loss: 0.01957 | val_logits_ll: 0.01932 |  0:05:10s\n",
      "\n",
      "Early stopping occured at epoch 125 with best_epoch = 105 and best_val_logits_ll = 0.01928\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD1_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD1_.pth.zip\n",
      "Seed:1,Fold:2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34091 | val_logits_ll: 0.03663 |  0:00:02s\n",
      "epoch 10 | loss: 0.02167 | val_logits_ll: 0.02145 |  0:00:27s\n",
      "epoch 20 | loss: 0.02034 | val_logits_ll: 0.0217  |  0:00:53s\n",
      "epoch 30 | loss: 0.01998 | val_logits_ll: 0.02006 |  0:01:19s\n",
      "epoch 40 | loss: 0.01982 | val_logits_ll: 0.01972 |  0:01:45s\n",
      "epoch 50 | loss: 0.01978 | val_logits_ll: 0.02054 |  0:02:11s\n",
      "epoch 60 | loss: 0.01973 | val_logits_ll: 0.01951 |  0:02:37s\n",
      "epoch 70 | loss: 0.01967 | val_logits_ll: 0.01944 |  0:03:02s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01939 |  0:03:28s\n",
      "epoch 90 | loss: 0.0196  | val_logits_ll: 0.01937 |  0:03:53s\n",
      "epoch 100| loss: 0.01961 | val_logits_ll: 0.01938 |  0:04:19s\n",
      "epoch 110| loss: 0.01959 | val_logits_ll: 0.01932 |  0:04:45s\n",
      "epoch 120| loss: 0.01967 | val_logits_ll: 0.01937 |  0:05:10s\n",
      "epoch 130| loss: 0.01957 | val_logits_ll: 0.01932 |  0:05:37s\n",
      "epoch 140| loss: 0.01954 | val_logits_ll: 0.0193  |  0:06:03s\n",
      "epoch 150| loss: 0.01952 | val_logits_ll: 0.01931 |  0:06:29s\n",
      "epoch 160| loss: 0.01951 | val_logits_ll: 0.01927 |  0:06:55s\n",
      "epoch 170| loss: 0.01949 | val_logits_ll: 0.01928 |  0:07:20s\n",
      "epoch 180| loss: 0.01948 | val_logits_ll: 0.01927 |  0:07:46s\n",
      "epoch 190| loss: 0.01948 | val_logits_ll: 0.0193  |  0:08:12s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 181 and best_val_logits_ll = 0.01924\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD2_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD2_.pth.zip\n",
      "Seed:1,Fold:3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34177 | val_logits_ll: 0.03691 |  0:00:02s\n",
      "epoch 10 | loss: 0.02123 | val_logits_ll: 0.02101 |  0:00:27s\n",
      "epoch 20 | loss: 0.02013 | val_logits_ll: 0.02217 |  0:00:53s\n",
      "epoch 30 | loss: 0.01993 | val_logits_ll: 0.02288 |  0:01:19s\n",
      "epoch 40 | loss: 0.01977 | val_logits_ll: 0.0201  |  0:01:45s\n",
      "epoch 50 | loss: 0.01972 | val_logits_ll: 0.01973 |  0:02:11s\n",
      "epoch 60 | loss: 0.01971 | val_logits_ll: 0.01974 |  0:02:37s\n",
      "epoch 70 | loss: 0.01968 | val_logits_ll: 0.01977 |  0:03:02s\n",
      "epoch 80 | loss: 0.01957 | val_logits_ll: 0.0197  |  0:03:29s\n",
      "epoch 90 | loss: 0.01951 | val_logits_ll: 0.01961 |  0:03:55s\n",
      "epoch 100| loss: 0.01955 | val_logits_ll: 0.01966 |  0:04:20s\n",
      "epoch 110| loss: 0.01953 | val_logits_ll: 0.0196  |  0:04:46s\n",
      "epoch 120| loss: 0.01948 | val_logits_ll: 0.01958 |  0:05:11s\n",
      "epoch 130| loss: 0.01946 | val_logits_ll: 0.01955 |  0:05:37s\n",
      "epoch 140| loss: 0.01946 | val_logits_ll: 0.01954 |  0:06:03s\n",
      "epoch 150| loss: 0.01943 | val_logits_ll: 0.01961 |  0:06:29s\n",
      "epoch 160| loss: 0.01945 | val_logits_ll: 0.01952 |  0:06:55s\n",
      "epoch 170| loss: 0.01945 | val_logits_ll: 0.01953 |  0:07:21s\n",
      "\n",
      "Early stopping occured at epoch 175 with best_epoch = 155 and best_val_logits_ll = 0.01951\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD3_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD3_.pth.zip\n",
      "Seed:1,Fold:4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34153 | val_logits_ll: 0.03692 |  0:00:02s\n",
      "epoch 10 | loss: 0.0217  | val_logits_ll: 0.0221  |  0:00:28s\n",
      "epoch 20 | loss: 0.02044 | val_logits_ll: 0.02026 |  0:00:54s\n",
      "epoch 30 | loss: 0.02018 | val_logits_ll: 0.01971 |  0:01:20s\n",
      "epoch 40 | loss: 0.02    | val_logits_ll: 0.0196  |  0:01:45s\n",
      "epoch 50 | loss: 0.0198  | val_logits_ll: 0.01938 |  0:02:11s\n",
      "epoch 60 | loss: 0.01976 | val_logits_ll: 0.01947 |  0:02:37s\n",
      "epoch 70 | loss: 0.01973 | val_logits_ll: 0.01938 |  0:03:04s\n",
      "epoch 80 | loss: 0.01969 | val_logits_ll: 0.01928 |  0:03:29s\n",
      "epoch 90 | loss: 0.01963 | val_logits_ll: 0.01932 |  0:03:55s\n",
      "epoch 100| loss: 0.01964 | val_logits_ll: 0.0193  |  0:04:21s\n",
      "epoch 110| loss: 0.0196  | val_logits_ll: 0.01923 |  0:04:46s\n",
      "epoch 120| loss: 0.01962 | val_logits_ll: 0.01922 |  0:05:12s\n",
      "epoch 130| loss: 0.0196  | val_logits_ll: 0.01922 |  0:05:38s\n",
      "epoch 140| loss: 0.01958 | val_logits_ll: 0.0192  |  0:06:04s\n",
      "epoch 150| loss: 0.01954 | val_logits_ll: 0.01918 |  0:06:29s\n",
      "epoch 160| loss: 0.01954 | val_logits_ll: 0.01919 |  0:06:55s\n",
      "epoch 170| loss: 0.01952 | val_logits_ll: 0.01916 |  0:07:21s\n",
      "epoch 180| loss: 0.01953 | val_logits_ll: 0.01916 |  0:07:48s\n",
      "epoch 190| loss: 0.01953 | val_logits_ll: 0.01916 |  0:08:13s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 187 and best_val_logits_ll = 0.01913\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD4_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD4_.pth.zip\n",
      "Seed:1,Fold:5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.33967 | val_logits_ll: 0.03777 |  0:00:02s\n",
      "epoch 10 | loss: 0.02162 | val_logits_ll: 0.02115 |  0:00:27s\n",
      "epoch 20 | loss: 0.02024 | val_logits_ll: 0.0201  |  0:00:53s\n",
      "epoch 30 | loss: 0.02002 | val_logits_ll: 0.01979 |  0:01:19s\n",
      "epoch 40 | loss: 0.01992 | val_logits_ll: 0.01972 |  0:01:45s\n",
      "epoch 50 | loss: 0.01985 | val_logits_ll: 0.01969 |  0:02:11s\n",
      "epoch 60 | loss: 0.01977 | val_logits_ll: 0.01948 |  0:02:37s\n",
      "epoch 70 | loss: 0.01969 | val_logits_ll: 0.01939 |  0:03:03s\n",
      "epoch 80 | loss: 0.01965 | val_logits_ll: 0.01943 |  0:03:29s\n",
      "epoch 90 | loss: 0.01966 | val_logits_ll: 0.01941 |  0:03:54s\n",
      "epoch 100| loss: 0.01961 | val_logits_ll: 0.01937 |  0:04:19s\n",
      "epoch 110| loss: 0.01955 | val_logits_ll: 0.01933 |  0:04:46s\n",
      "epoch 120| loss: 0.01956 | val_logits_ll: 0.01938 |  0:05:12s\n",
      "epoch 130| loss: 0.01952 | val_logits_ll: 0.01933 |  0:05:38s\n",
      "epoch 140| loss: 0.01955 | val_logits_ll: 0.01931 |  0:06:04s\n",
      "epoch 150| loss: 0.01951 | val_logits_ll: 0.01935 |  0:06:28s\n",
      "epoch 160| loss: 0.01951 | val_logits_ll: 0.01932 |  0:06:54s\n",
      "epoch 170| loss: 0.01948 | val_logits_ll: 0.01932 |  0:07:20s\n",
      "epoch 180| loss: 0.01949 | val_logits_ll: 0.01932 |  0:07:46s\n",
      "\n",
      "Early stopping occured at epoch 181 with best_epoch = 161 and best_val_logits_ll = 0.01929\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD5_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD5_.pth.zip\n",
      "Seed:1,Fold:6\n",
      "Device used : cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.34205 | val_logits_ll: 0.03669 |  0:00:02s\n",
      "epoch 10 | loss: 0.02151 | val_logits_ll: 0.02145 |  0:00:28s\n",
      "epoch 20 | loss: 0.02019 | val_logits_ll: 0.02041 |  0:00:54s\n",
      "epoch 30 | loss: 0.02001 | val_logits_ll: 0.02021 |  0:01:20s\n",
      "epoch 40 | loss: 0.0199  | val_logits_ll: 0.01976 |  0:01:45s\n",
      "epoch 50 | loss: 0.01982 | val_logits_ll: 0.01961 |  0:02:10s\n",
      "epoch 60 | loss: 0.01977 | val_logits_ll: 0.01946 |  0:02:36s\n",
      "epoch 70 | loss: 0.0197  | val_logits_ll: 0.01948 |  0:03:01s\n",
      "epoch 80 | loss: 0.01962 | val_logits_ll: 0.01942 |  0:03:27s\n",
      "epoch 90 | loss: 0.01962 | val_logits_ll: 0.01944 |  0:03:53s\n",
      "epoch 100| loss: 0.01959 | val_logits_ll: 0.01941 |  0:04:17s\n",
      "epoch 110| loss: 0.01975 | val_logits_ll: 0.01956 |  0:04:43s\n",
      "epoch 120| loss: 0.01957 | val_logits_ll: 0.01938 |  0:05:09s\n",
      "epoch 130| loss: 0.01955 | val_logits_ll: 0.01939 |  0:05:35s\n",
      "epoch 140| loss: 0.01953 | val_logits_ll: 0.01937 |  0:06:00s\n",
      "epoch 150| loss: 0.01954 | val_logits_ll: 0.01939 |  0:06:27s\n",
      "epoch 160| loss: 0.01951 | val_logits_ll: 0.01934 |  0:06:52s\n",
      "epoch 170| loss: 0.0195  | val_logits_ll: 0.01932 |  0:07:18s\n",
      "epoch 180| loss: 0.0195  | val_logits_ll: 0.01932 |  0:07:43s\n",
      "epoch 190| loss: 0.0195  | val_logits_ll: 0.01932 |  0:08:09s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 189 and best_val_logits_ll = 0.0193\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD6_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD6_.pth.zip\n",
      "Seed:2,Fold:0\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34697 | val_logits_ll: 0.03674 |  0:00:02s\n",
      "epoch 10 | loss: 0.02112 | val_logits_ll: 0.02274 |  0:00:27s\n",
      "epoch 20 | loss: 0.02028 | val_logits_ll: 0.02197 |  0:00:53s\n",
      "epoch 30 | loss: 0.01998 | val_logits_ll: 0.02125 |  0:01:18s\n",
      "epoch 40 | loss: 0.01993 | val_logits_ll: 0.02005 |  0:01:44s\n",
      "epoch 50 | loss: 0.01985 | val_logits_ll: 0.01992 |  0:02:10s\n",
      "epoch 60 | loss: 0.0197  | val_logits_ll: 0.01987 |  0:02:35s\n",
      "epoch 70 | loss: 0.01964 | val_logits_ll: 0.0196  |  0:03:01s\n",
      "epoch 80 | loss: 0.01962 | val_logits_ll: 0.01965 |  0:03:27s\n",
      "epoch 90 | loss: 0.01956 | val_logits_ll: 0.01955 |  0:03:53s\n",
      "epoch 100| loss: 0.01963 | val_logits_ll: 0.01955 |  0:04:19s\n",
      "epoch 110| loss: 0.01952 | val_logits_ll: 0.01953 |  0:04:45s\n",
      "epoch 120| loss: 0.0195  | val_logits_ll: 0.01951 |  0:05:10s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.01948 |  0:05:36s\n",
      "epoch 140| loss: 0.0195  | val_logits_ll: 0.01948 |  0:06:02s\n",
      "epoch 150| loss: 0.01951 | val_logits_ll: 0.01947 |  0:06:27s\n",
      "epoch 160| loss: 0.01948 | val_logits_ll: 0.01948 |  0:06:53s\n",
      "epoch 170| loss: 0.01946 | val_logits_ll: 0.01946 |  0:07:19s\n",
      "epoch 180| loss: 0.01946 | val_logits_ll: 0.01946 |  0:07:45s\n",
      "epoch 190| loss: 0.01945 | val_logits_ll: 0.01946 |  0:08:11s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 193 and best_val_logits_ll = 0.01943\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD0_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD0_.pth.zip\n",
      "Seed:2,Fold:1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34622 | val_logits_ll: 0.03664 |  0:00:02s\n",
      "epoch 10 | loss: 0.0214  | val_logits_ll: 0.02188 |  0:00:28s\n",
      "epoch 20 | loss: 0.02025 | val_logits_ll: 0.02047 |  0:00:54s\n",
      "epoch 30 | loss: 0.02003 | val_logits_ll: 0.02159 |  0:01:19s\n",
      "epoch 40 | loss: 0.01991 | val_logits_ll: 0.01968 |  0:01:45s\n",
      "epoch 50 | loss: 0.01985 | val_logits_ll: 0.01958 |  0:02:11s\n",
      "epoch 60 | loss: 0.01981 | val_logits_ll: 0.01962 |  0:02:37s\n",
      "epoch 70 | loss: 0.01969 | val_logits_ll: 0.01946 |  0:03:03s\n",
      "epoch 80 | loss: 0.01972 | val_logits_ll: 0.01941 |  0:03:29s\n",
      "epoch 90 | loss: 0.01973 | val_logits_ll: 0.01944 |  0:03:55s\n",
      "epoch 100| loss: 0.01965 | val_logits_ll: 0.01938 |  0:04:21s\n",
      "epoch 110| loss: 0.01967 | val_logits_ll: 0.01935 |  0:04:47s\n",
      "epoch 120| loss: 0.01962 | val_logits_ll: 0.01932 |  0:05:13s\n",
      "epoch 130| loss: 0.0196  | val_logits_ll: 0.01931 |  0:05:39s\n",
      "epoch 140| loss: 0.01958 | val_logits_ll: 0.01929 |  0:06:05s\n",
      "epoch 150| loss: 0.01956 | val_logits_ll: 0.01931 |  0:06:31s\n",
      "epoch 160| loss: 0.01954 | val_logits_ll: 0.01927 |  0:06:57s\n",
      "epoch 170| loss: 0.01952 | val_logits_ll: 0.01923 |  0:07:23s\n",
      "epoch 180| loss: 0.01952 | val_logits_ll: 0.01926 |  0:07:48s\n",
      "epoch 190| loss: 0.01949 | val_logits_ll: 0.01926 |  0:08:14s\n",
      "\n",
      "Early stopping occured at epoch 190 with best_epoch = 170 and best_val_logits_ll = 0.01923\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD1_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD1_.pth.zip\n",
      "Seed:2,Fold:2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.35053 | val_logits_ll: 0.0403  |  0:00:03s\n",
      "epoch 10 | loss: 0.02138 | val_logits_ll: 0.02197 |  0:00:29s\n",
      "epoch 20 | loss: 0.02031 | val_logits_ll: 0.02193 |  0:00:55s\n",
      "epoch 30 | loss: 0.02001 | val_logits_ll: 0.01986 |  0:01:21s\n",
      "epoch 40 | loss: 0.01986 | val_logits_ll: 0.01976 |  0:01:47s\n",
      "epoch 50 | loss: 0.01977 | val_logits_ll: 0.0196  |  0:02:13s\n",
      "epoch 60 | loss: 0.01969 | val_logits_ll: 0.01941 |  0:02:38s\n",
      "epoch 70 | loss: 0.01966 | val_logits_ll: 0.01943 |  0:03:05s\n",
      "epoch 80 | loss: 0.01959 | val_logits_ll: 0.01931 |  0:03:30s\n",
      "epoch 90 | loss: 0.01958 | val_logits_ll: 0.01935 |  0:03:56s\n",
      "epoch 100| loss: 0.01956 | val_logits_ll: 0.01931 |  0:04:22s\n",
      "epoch 110| loss: 0.01956 | val_logits_ll: 0.01925 |  0:04:48s\n",
      "epoch 120| loss: 0.01955 | val_logits_ll: 0.01925 |  0:05:13s\n",
      "epoch 130| loss: 0.01952 | val_logits_ll: 0.01923 |  0:05:39s\n",
      "epoch 140| loss: 0.01952 | val_logits_ll: 0.01924 |  0:06:05s\n",
      "epoch 150| loss: 0.01949 | val_logits_ll: 0.01927 |  0:06:31s\n",
      "\n",
      "Early stopping occured at epoch 158 with best_epoch = 138 and best_val_logits_ll = 0.0192\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD2_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD2_.pth.zip\n",
      "Seed:2,Fold:3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3492  | val_logits_ll: 0.03772 |  0:00:02s\n",
      "epoch 10 | loss: 0.02118 | val_logits_ll: 0.02122 |  0:00:27s\n",
      "epoch 20 | loss: 0.02024 | val_logits_ll: 0.0217  |  0:00:54s\n",
      "epoch 30 | loss: 0.01988 | val_logits_ll: 0.02204 |  0:01:19s\n",
      "epoch 40 | loss: 0.01976 | val_logits_ll: 0.01981 |  0:01:45s\n",
      "epoch 50 | loss: 0.01968 | val_logits_ll: 0.01964 |  0:02:11s\n",
      "epoch 60 | loss: 0.01966 | val_logits_ll: 0.01969 |  0:02:37s\n",
      "epoch 70 | loss: 0.01967 | val_logits_ll: 0.02008 |  0:03:03s\n",
      "epoch 80 | loss: 0.01963 | val_logits_ll: 0.01971 |  0:03:29s\n",
      "epoch 90 | loss: 0.01965 | val_logits_ll: 0.01962 |  0:03:54s\n",
      "epoch 100| loss: 0.01956 | val_logits_ll: 0.01958 |  0:04:21s\n",
      "epoch 110| loss: 0.01955 | val_logits_ll: 0.01954 |  0:04:47s\n",
      "epoch 120| loss: 0.01953 | val_logits_ll: 0.01951 |  0:05:13s\n",
      "epoch 130| loss: 0.01951 | val_logits_ll: 0.0195  |  0:05:39s\n",
      "epoch 140| loss: 0.01953 | val_logits_ll: 0.01956 |  0:06:05s\n",
      "epoch 150| loss: 0.01948 | val_logits_ll: 0.0195  |  0:06:32s\n",
      "\n",
      "Early stopping occured at epoch 156 with best_epoch = 136 and best_val_logits_ll = 0.01945\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD3_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD3_.pth.zip\n",
      "Seed:2,Fold:4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.35098 | val_logits_ll: 0.03776 |  0:00:02s\n",
      "epoch 10 | loss: 0.02142 | val_logits_ll: 0.0219  |  0:00:28s\n",
      "epoch 20 | loss: 0.02025 | val_logits_ll: 0.02022 |  0:00:54s\n",
      "epoch 30 | loss: 0.01993 | val_logits_ll: 0.02016 |  0:01:20s\n",
      "epoch 40 | loss: 0.01981 | val_logits_ll: 0.0199  |  0:01:46s\n",
      "epoch 50 | loss: 0.01979 | val_logits_ll: 0.01992 |  0:02:13s\n",
      "epoch 60 | loss: 0.01973 | val_logits_ll: 0.0196  |  0:02:39s\n",
      "epoch 70 | loss: 0.01989 | val_logits_ll: 0.01968 |  0:03:05s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01945 |  0:03:31s\n",
      "epoch 90 | loss: 0.0196  | val_logits_ll: 0.01935 |  0:03:57s\n",
      "epoch 100| loss: 0.01965 | val_logits_ll: 0.01945 |  0:04:24s\n",
      "epoch 110| loss: 0.0196  | val_logits_ll: 0.01938 |  0:04:49s\n",
      "epoch 120| loss: 0.01956 | val_logits_ll: 0.01936 |  0:05:15s\n",
      "epoch 130| loss: 0.01955 | val_logits_ll: 0.01936 |  0:05:43s\n",
      "epoch 140| loss: 0.01952 | val_logits_ll: 0.01934 |  0:06:08s\n",
      "epoch 150| loss: 0.01951 | val_logits_ll: 0.01932 |  0:06:34s\n",
      "epoch 160| loss: 0.01951 | val_logits_ll: 0.01932 |  0:07:00s\n",
      "epoch 170| loss: 0.01952 | val_logits_ll: 0.01935 |  0:07:26s\n",
      "epoch 180| loss: 0.01947 | val_logits_ll: 0.01927 |  0:07:52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 190| loss: 0.01948 | val_logits_ll: 0.01929 |  0:08:18s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 187 and best_val_logits_ll = 0.01926\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD4_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD4_.pth.zip\n",
      "Seed:2,Fold:5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.35218 | val_logits_ll: 0.03947 |  0:00:02s\n",
      "epoch 10 | loss: 0.02142 | val_logits_ll: 0.02106 |  0:00:28s\n",
      "epoch 20 | loss: 0.02021 | val_logits_ll: 0.01988 |  0:00:54s\n",
      "epoch 30 | loss: 0.01995 | val_logits_ll: 0.01955 |  0:01:20s\n",
      "epoch 40 | loss: 0.01985 | val_logits_ll: 0.01935 |  0:01:45s\n",
      "epoch 50 | loss: 0.01989 | val_logits_ll: 0.01947 |  0:02:10s\n",
      "epoch 60 | loss: 0.01977 | val_logits_ll: 0.01923 |  0:02:37s\n",
      "epoch 70 | loss: 0.0197  | val_logits_ll: 0.01923 |  0:03:03s\n",
      "epoch 80 | loss: 0.01964 | val_logits_ll: 0.01915 |  0:03:28s\n",
      "epoch 90 | loss: 0.01967 | val_logits_ll: 0.01924 |  0:03:54s\n",
      "\n",
      "Early stopping occured at epoch 91 with best_epoch = 71 and best_val_logits_ll = 0.01913\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD5_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD5_.pth.zip\n",
      "Seed:2,Fold:6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34972 | val_logits_ll: 0.03555 |  0:00:02s\n",
      "epoch 10 | loss: 0.02146 | val_logits_ll: 0.02092 |  0:00:31s\n",
      "epoch 20 | loss: 0.02011 | val_logits_ll: 0.02424 |  0:01:00s\n",
      "epoch 30 | loss: 0.01991 | val_logits_ll: 0.01975 |  0:01:28s\n",
      "epoch 40 | loss: 0.0198  | val_logits_ll: 0.01974 |  0:01:57s\n",
      "epoch 50 | loss: 0.01974 | val_logits_ll: 0.0196  |  0:02:27s\n",
      "epoch 60 | loss: 0.01972 | val_logits_ll: 0.0197  |  0:02:55s\n",
      "epoch 70 | loss: 0.01987 | val_logits_ll: 0.0199  |  0:03:23s\n",
      "epoch 80 | loss: 0.01961 | val_logits_ll: 0.01953 |  0:03:51s\n",
      "epoch 90 | loss: 0.01962 | val_logits_ll: 0.01951 |  0:04:20s\n",
      "epoch 100| loss: 0.01958 | val_logits_ll: 0.01939 |  0:04:49s\n",
      "epoch 110| loss: 0.01957 | val_logits_ll: 0.0194  |  0:05:17s\n",
      "epoch 120| loss: 0.01958 | val_logits_ll: 0.01943 |  0:05:45s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.0194  |  0:06:13s\n",
      "epoch 140| loss: 0.01953 | val_logits_ll: 0.01939 |  0:06:42s\n",
      "epoch 150| loss: 0.0195  | val_logits_ll: 0.01936 |  0:07:10s\n",
      "epoch 160| loss: 0.01948 | val_logits_ll: 0.01938 |  0:07:39s\n",
      "epoch 170| loss: 0.01947 | val_logits_ll: 0.01936 |  0:08:08s\n",
      "epoch 180| loss: 0.01946 | val_logits_ll: 0.01937 |  0:08:37s\n",
      "epoch 190| loss: 0.01945 | val_logits_ll: 0.01935 |  0:09:05s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 193 and best_val_logits_ll = 0.01931\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD6_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD6_.pth.zip\n",
      "Seed:3,Fold:0\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32173 | val_logits_ll: 0.03269 |  0:00:02s\n",
      "epoch 10 | loss: 0.02109 | val_logits_ll: 0.02153 |  0:00:30s\n",
      "epoch 20 | loss: 0.02013 | val_logits_ll: 0.02331 |  0:01:00s\n",
      "epoch 30 | loss: 0.01995 | val_logits_ll: 0.01983 |  0:01:28s\n",
      "epoch 40 | loss: 0.01988 | val_logits_ll: 0.01967 |  0:01:57s\n",
      "epoch 50 | loss: 0.02008 | val_logits_ll: 0.01971 |  0:02:27s\n",
      "epoch 60 | loss: 0.01974 | val_logits_ll: 0.01941 |  0:02:56s\n",
      "epoch 70 | loss: 0.01973 | val_logits_ll: 0.0194  |  0:03:23s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01931 |  0:03:52s\n",
      "epoch 90 | loss: 0.01964 | val_logits_ll: 0.01931 |  0:04:19s\n",
      "epoch 100| loss: 0.01961 | val_logits_ll: 0.01927 |  0:04:48s\n",
      "epoch 110| loss: 0.01958 | val_logits_ll: 0.01924 |  0:05:16s\n",
      "epoch 120| loss: 0.01957 | val_logits_ll: 0.01925 |  0:05:44s\n",
      "epoch 130| loss: 0.01959 | val_logits_ll: 0.01923 |  0:06:12s\n",
      "epoch 140| loss: 0.01954 | val_logits_ll: 0.01927 |  0:06:41s\n",
      "epoch 150| loss: 0.01954 | val_logits_ll: 0.01929 |  0:07:08s\n",
      "epoch 160| loss: 0.01952 | val_logits_ll: 0.01927 |  0:07:36s\n",
      "epoch 170| loss: 0.01951 | val_logits_ll: 0.01922 |  0:08:05s\n",
      "epoch 180| loss: 0.0195  | val_logits_ll: 0.01922 |  0:08:35s\n",
      "epoch 190| loss: 0.0195  | val_logits_ll: 0.01919 |  0:09:02s\n",
      "\n",
      "Early stopping occured at epoch 192 with best_epoch = 172 and best_val_logits_ll = 0.01917\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD0_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD0_.pth.zip\n",
      "Seed:3,Fold:1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32925 | val_logits_ll: 0.03488 |  0:00:02s\n",
      "epoch 10 | loss: 0.02101 | val_logits_ll: 0.02214 |  0:00:28s\n",
      "epoch 20 | loss: 0.02016 | val_logits_ll: 0.01969 |  0:00:53s\n",
      "epoch 30 | loss: 0.01999 | val_logits_ll: 0.01949 |  0:01:20s\n",
      "epoch 40 | loss: 0.01982 | val_logits_ll: 0.01939 |  0:01:45s\n",
      "epoch 50 | loss: 0.01979 | val_logits_ll: 0.01933 |  0:02:10s\n",
      "epoch 60 | loss: 0.0197  | val_logits_ll: 0.01923 |  0:02:35s\n",
      "epoch 70 | loss: 0.0197  | val_logits_ll: 0.01927 |  0:03:02s\n",
      "epoch 80 | loss: 0.01965 | val_logits_ll: 0.01924 |  0:03:27s\n",
      "epoch 90 | loss: 0.01964 | val_logits_ll: 0.01919 |  0:03:53s\n",
      "epoch 100| loss: 0.01963 | val_logits_ll: 0.01921 |  0:04:19s\n",
      "epoch 110| loss: 0.01963 | val_logits_ll: 0.0192  |  0:04:45s\n",
      "\n",
      "Early stopping occured at epoch 112 with best_epoch = 92 and best_val_logits_ll = 0.01914\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD1_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD1_.pth.zip\n",
      "Seed:3,Fold:2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32738 | val_logits_ll: 0.03366 |  0:00:02s\n",
      "epoch 10 | loss: 0.02118 | val_logits_ll: 0.02352 |  0:00:28s\n",
      "epoch 20 | loss: 0.02022 | val_logits_ll: 0.0246  |  0:00:54s\n",
      "epoch 30 | loss: 0.01992 | val_logits_ll: 0.01972 |  0:01:20s\n",
      "epoch 40 | loss: 0.01982 | val_logits_ll: 0.01966 |  0:01:46s\n",
      "epoch 50 | loss: 0.01975 | val_logits_ll: 0.01987 |  0:02:13s\n",
      "epoch 60 | loss: 0.01974 | val_logits_ll: 0.01949 |  0:02:38s\n",
      "epoch 70 | loss: 0.01966 | val_logits_ll: 0.01936 |  0:03:04s\n",
      "epoch 80 | loss: 0.01973 | val_logits_ll: 0.01942 |  0:03:30s\n",
      "epoch 90 | loss: 0.01963 | val_logits_ll: 0.01937 |  0:03:56s\n",
      "\n",
      "Early stopping occured at epoch 95 with best_epoch = 75 and best_val_logits_ll = 0.01934\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD2_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD2_.pth.zip\n",
      "Seed:3,Fold:3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.33005 | val_logits_ll: 0.036   |  0:00:02s\n",
      "epoch 10 | loss: 0.02141 | val_logits_ll: 0.02259 |  0:00:27s\n",
      "epoch 20 | loss: 0.02016 | val_logits_ll: 0.02003 |  0:00:53s\n",
      "epoch 30 | loss: 0.01991 | val_logits_ll: 0.01972 |  0:01:19s\n",
      "epoch 40 | loss: 0.01987 | val_logits_ll: 0.01977 |  0:01:44s\n",
      "epoch 50 | loss: 0.01979 | val_logits_ll: 0.01954 |  0:02:10s\n",
      "epoch 60 | loss: 0.01973 | val_logits_ll: 0.01956 |  0:02:35s\n",
      "epoch 70 | loss: 0.01967 | val_logits_ll: 0.01948 |  0:03:00s\n",
      "epoch 80 | loss: 0.01965 | val_logits_ll: 0.01946 |  0:03:26s\n",
      "epoch 90 | loss: 0.01966 | val_logits_ll: 0.01948 |  0:03:52s\n",
      "epoch 100| loss: 0.01959 | val_logits_ll: 0.01947 |  0:04:17s\n",
      "epoch 110| loss: 0.01956 | val_logits_ll: 0.01942 |  0:04:43s\n",
      "epoch 120| loss: 0.01955 | val_logits_ll: 0.01941 |  0:05:09s\n",
      "epoch 130| loss: 0.01957 | val_logits_ll: 0.0195  |  0:05:35s\n",
      "epoch 140| loss: 0.01949 | val_logits_ll: 0.0194  |  0:06:01s\n",
      "epoch 150| loss: 0.01951 | val_logits_ll: 0.01939 |  0:06:27s\n",
      "\n",
      "Early stopping occured at epoch 159 with best_epoch = 139 and best_val_logits_ll = 0.01937\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD3_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD3_.pth.zip\n",
      "Seed:3,Fold:4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32688 | val_logits_ll: 0.03511 |  0:00:02s\n",
      "epoch 10 | loss: 0.02121 | val_logits_ll: 0.02105 |  0:00:28s\n",
      "epoch 20 | loss: 0.02017 | val_logits_ll: 0.02277 |  0:00:54s\n",
      "epoch 30 | loss: 0.02005 | val_logits_ll: 0.02044 |  0:01:19s\n",
      "epoch 40 | loss: 0.01985 | val_logits_ll: 0.01994 |  0:01:45s\n",
      "epoch 50 | loss: 0.01977 | val_logits_ll: 0.0197  |  0:02:10s\n",
      "epoch 60 | loss: 0.01966 | val_logits_ll: 0.0196  |  0:02:35s\n",
      "epoch 70 | loss: 0.01968 | val_logits_ll: 0.01956 |  0:03:01s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01953 |  0:03:27s\n",
      "epoch 90 | loss: 0.01968 | val_logits_ll: 0.01958 |  0:03:52s\n",
      "epoch 100| loss: 0.01961 | val_logits_ll: 0.01951 |  0:04:18s\n",
      "\n",
      "Early stopping occured at epoch 101 with best_epoch = 81 and best_val_logits_ll = 0.01949\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saving: tabnet_v2_FOLD4_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD4_.pth.zip\n",
      "Seed:3,Fold:5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32521 | val_logits_ll: 0.03403 |  0:00:02s\n",
      "epoch 10 | loss: 0.02167 | val_logits_ll: 0.02125 |  0:00:27s\n",
      "epoch 20 | loss: 0.02029 | val_logits_ll: 0.02033 |  0:00:54s\n",
      "epoch 30 | loss: 0.02    | val_logits_ll: 0.01998 |  0:01:18s\n",
      "epoch 40 | loss: 0.01985 | val_logits_ll: 0.01965 |  0:01:44s\n",
      "epoch 50 | loss: 0.01977 | val_logits_ll: 0.01956 |  0:02:10s\n",
      "epoch 60 | loss: 0.01971 | val_logits_ll: 0.01969 |  0:02:35s\n",
      "epoch 70 | loss: 0.01972 | val_logits_ll: 0.01966 |  0:03:01s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01951 |  0:03:26s\n",
      "epoch 90 | loss: 0.01961 | val_logits_ll: 0.01945 |  0:03:51s\n",
      "epoch 100| loss: 0.0196  | val_logits_ll: 0.01945 |  0:04:17s\n",
      "epoch 110| loss: 0.01959 | val_logits_ll: 0.0196  |  0:04:42s\n",
      "epoch 120| loss: 0.01961 | val_logits_ll: 0.01961 |  0:05:07s\n",
      "epoch 130| loss: 0.01961 | val_logits_ll: 0.01943 |  0:05:32s\n",
      "epoch 140| loss: 0.01956 | val_logits_ll: 0.0194  |  0:05:58s\n",
      "epoch 150| loss: 0.01953 | val_logits_ll: 0.01936 |  0:06:23s\n",
      "epoch 160| loss: 0.01952 | val_logits_ll: 0.01939 |  0:06:49s\n",
      "epoch 170| loss: 0.01953 | val_logits_ll: 0.01937 |  0:07:14s\n",
      "\n",
      "Early stopping occured at epoch 170 with best_epoch = 150 and best_val_logits_ll = 0.01936\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD5_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD5_.pth.zip\n",
      "Seed:3,Fold:6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.32971 | val_logits_ll: 0.0356  |  0:00:02s\n",
      "epoch 10 | loss: 0.02177 | val_logits_ll: 0.02145 |  0:00:28s\n",
      "epoch 20 | loss: 0.02016 | val_logits_ll: 0.02261 |  0:00:54s\n",
      "epoch 30 | loss: 0.01984 | val_logits_ll: 0.01978 |  0:01:20s\n",
      "epoch 40 | loss: 0.01984 | val_logits_ll: 0.02071 |  0:01:45s\n",
      "epoch 50 | loss: 0.01981 | val_logits_ll: 0.01963 |  0:02:11s\n",
      "epoch 60 | loss: 0.01971 | val_logits_ll: 0.01984 |  0:02:37s\n",
      "epoch 70 | loss: 0.01968 | val_logits_ll: 0.01965 |  0:03:02s\n",
      "epoch 80 | loss: 0.01974 | val_logits_ll: 0.0196  |  0:03:27s\n",
      "epoch 90 | loss: 0.01963 | val_logits_ll: 0.01956 |  0:03:53s\n",
      "epoch 100| loss: 0.0197  | val_logits_ll: 0.01961 |  0:04:19s\n",
      "epoch 110| loss: 0.01961 | val_logits_ll: 0.01955 |  0:04:44s\n",
      "epoch 120| loss: 0.01956 | val_logits_ll: 0.0195  |  0:05:10s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.01951 |  0:05:35s\n",
      "epoch 140| loss: 0.01955 | val_logits_ll: 0.01957 |  0:06:02s\n",
      "epoch 150| loss: 0.01951 | val_logits_ll: 0.01946 |  0:06:28s\n",
      "epoch 160| loss: 0.01949 | val_logits_ll: 0.01958 |  0:06:54s\n",
      "epoch 170| loss: 0.01946 | val_logits_ll: 0.01945 |  0:07:20s\n",
      "epoch 180| loss: 0.01946 | val_logits_ll: 0.01946 |  0:07:45s\n",
      "epoch 190| loss: 0.01947 | val_logits_ll: 0.01943 |  0:08:11s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 196 and best_val_logits_ll = 0.01942\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD6_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD6_.pth.zip\n",
      "Seed:4,Fold:0\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34139 | val_logits_ll: 0.0348  |  0:00:02s\n",
      "epoch 10 | loss: 0.02139 | val_logits_ll: 0.02286 |  0:00:28s\n",
      "epoch 20 | loss: 0.02022 | val_logits_ll: 0.02193 |  0:00:53s\n",
      "epoch 30 | loss: 0.01997 | val_logits_ll: 0.01993 |  0:01:19s\n",
      "epoch 40 | loss: 0.01984 | val_logits_ll: 0.01967 |  0:01:44s\n",
      "epoch 50 | loss: 0.01978 | val_logits_ll: 0.01956 |  0:02:09s\n",
      "epoch 60 | loss: 0.01977 | val_logits_ll: 0.01952 |  0:02:35s\n",
      "epoch 70 | loss: 0.0197  | val_logits_ll: 0.01954 |  0:03:01s\n",
      "epoch 80 | loss: 0.01968 | val_logits_ll: 0.01939 |  0:03:27s\n",
      "epoch 90 | loss: 0.01963 | val_logits_ll: 0.01943 |  0:03:52s\n",
      "epoch 100| loss: 0.01964 | val_logits_ll: 0.01943 |  0:04:18s\n",
      "\n",
      "Early stopping occured at epoch 101 with best_epoch = 81 and best_val_logits_ll = 0.01937\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD0_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD0_.pth.zip\n",
      "Seed:4,Fold:1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34907 | val_logits_ll: 0.03969 |  0:00:02s\n",
      "epoch 10 | loss: 0.02149 | val_logits_ll: 0.02332 |  0:00:27s\n",
      "epoch 20 | loss: 0.02011 | val_logits_ll: 0.02346 |  0:00:53s\n",
      "epoch 30 | loss: 0.01992 | val_logits_ll: 0.01972 |  0:01:18s\n",
      "epoch 40 | loss: 0.0198  | val_logits_ll: 0.01965 |  0:01:44s\n",
      "epoch 50 | loss: 0.01973 | val_logits_ll: 0.0197  |  0:02:09s\n",
      "epoch 60 | loss: 0.01973 | val_logits_ll: 0.01962 |  0:02:34s\n",
      "epoch 70 | loss: 0.01966 | val_logits_ll: 0.01949 |  0:03:00s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01951 |  0:03:26s\n",
      "epoch 90 | loss: 0.01968 | val_logits_ll: 0.0195  |  0:03:52s\n",
      "epoch 100| loss: 0.01964 | val_logits_ll: 0.01945 |  0:04:17s\n",
      "epoch 110| loss: 0.01959 | val_logits_ll: 0.01952 |  0:04:44s\n",
      "epoch 120| loss: 0.01958 | val_logits_ll: 0.01941 |  0:05:09s\n",
      "\n",
      "Early stopping occured at epoch 128 with best_epoch = 108 and best_val_logits_ll = 0.01938\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD1_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD1_.pth.zip\n",
      "Seed:4,Fold:2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34918 | val_logits_ll: 0.03438 |  0:00:02s\n",
      "epoch 10 | loss: 0.02151 | val_logits_ll: 0.02106 |  0:00:28s\n",
      "epoch 20 | loss: 0.02015 | val_logits_ll: 0.02179 |  0:00:54s\n",
      "epoch 30 | loss: 0.02    | val_logits_ll: 0.02037 |  0:01:19s\n",
      "epoch 40 | loss: 0.01983 | val_logits_ll: 0.01949 |  0:01:45s\n",
      "epoch 50 | loss: 0.0198  | val_logits_ll: 0.0195  |  0:02:11s\n",
      "epoch 60 | loss: 0.01976 | val_logits_ll: 0.01941 |  0:02:37s\n",
      "epoch 70 | loss: 0.01986 | val_logits_ll: 0.01951 |  0:03:03s\n",
      "epoch 80 | loss: 0.01969 | val_logits_ll: 0.0194  |  0:03:28s\n",
      "epoch 90 | loss: 0.0197  | val_logits_ll: 0.0193  |  0:03:53s\n",
      "epoch 100| loss: 0.01968 | val_logits_ll: 0.01942 |  0:04:18s\n",
      "epoch 110| loss: 0.01965 | val_logits_ll: 0.01929 |  0:04:45s\n",
      "epoch 120| loss: 0.01972 | val_logits_ll: 0.01931 |  0:05:11s\n",
      "epoch 130| loss: 0.0196  | val_logits_ll: 0.0193  |  0:05:37s\n",
      "epoch 140| loss: 0.0196  | val_logits_ll: 0.01926 |  0:06:02s\n",
      "epoch 150| loss: 0.01966 | val_logits_ll: 0.0193  |  0:06:29s\n",
      "epoch 160| loss: 0.01961 | val_logits_ll: 0.01927 |  0:06:54s\n",
      "\n",
      "Early stopping occured at epoch 168 with best_epoch = 148 and best_val_logits_ll = 0.01922\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD2_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD2_.pth.zip\n",
      "Seed:4,Fold:3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34388 | val_logits_ll: 0.03552 |  0:00:02s\n",
      "epoch 10 | loss: 0.02142 | val_logits_ll: 0.022   |  0:00:28s\n",
      "epoch 20 | loss: 0.02015 | val_logits_ll: 0.0223  |  0:00:53s\n",
      "epoch 30 | loss: 0.01992 | val_logits_ll: 0.02018 |  0:01:18s\n",
      "epoch 40 | loss: 0.01982 | val_logits_ll: 0.02037 |  0:01:44s\n",
      "epoch 50 | loss: 0.01972 | val_logits_ll: 0.01958 |  0:02:09s\n",
      "epoch 60 | loss: 0.01968 | val_logits_ll: 0.01955 |  0:02:35s\n",
      "epoch 70 | loss: 0.01967 | val_logits_ll: 0.01955 |  0:03:01s\n",
      "epoch 80 | loss: 0.01967 | val_logits_ll: 0.01952 |  0:03:26s\n",
      "epoch 90 | loss: 0.01958 | val_logits_ll: 0.01942 |  0:03:52s\n",
      "epoch 100| loss: 0.01957 | val_logits_ll: 0.01952 |  0:04:17s\n",
      "epoch 110| loss: 0.01956 | val_logits_ll: 0.01952 |  0:04:44s\n",
      "\n",
      "Early stopping occured at epoch 110 with best_epoch = 90 and best_val_logits_ll = 0.01942\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD3_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD3_.pth.zip\n",
      "Seed:4,Fold:4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34709 | val_logits_ll: 0.04064 |  0:00:02s\n",
      "epoch 10 | loss: 0.02152 | val_logits_ll: 0.0212  |  0:00:27s\n",
      "epoch 20 | loss: 0.02033 | val_logits_ll: 0.02186 |  0:00:53s\n",
      "epoch 30 | loss: 0.02012 | val_logits_ll: 0.01991 |  0:01:18s\n",
      "epoch 40 | loss: 0.02    | val_logits_ll: 0.01981 |  0:01:44s\n",
      "epoch 50 | loss: 0.01981 | val_logits_ll: 0.01961 |  0:02:10s\n",
      "epoch 60 | loss: 0.01979 | val_logits_ll: 0.01947 |  0:02:36s\n",
      "epoch 70 | loss: 0.01977 | val_logits_ll: 0.01957 |  0:03:02s\n",
      "epoch 80 | loss: 0.01975 | val_logits_ll: 0.01949 |  0:03:27s\n",
      "epoch 90 | loss: 0.01966 | val_logits_ll: 0.01941 |  0:03:53s\n",
      "epoch 100| loss: 0.01969 | val_logits_ll: 0.01946 |  0:04:20s\n",
      "epoch 110| loss: 0.01961 | val_logits_ll: 0.01944 |  0:04:45s\n",
      "epoch 120| loss: 0.01962 | val_logits_ll: 0.01939 |  0:05:10s\n",
      "epoch 130| loss: 0.01956 | val_logits_ll: 0.01933 |  0:05:37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 140| loss: 0.0196  | val_logits_ll: 0.01944 |  0:06:03s\n",
      "epoch 150| loss: 0.01956 | val_logits_ll: 0.01933 |  0:06:29s\n",
      "epoch 160| loss: 0.01958 | val_logits_ll: 0.01933 |  0:06:55s\n",
      "epoch 170| loss: 0.01954 | val_logits_ll: 0.01932 |  0:07:22s\n",
      "epoch 180| loss: 0.01954 | val_logits_ll: 0.01931 |  0:07:48s\n",
      "\n",
      "Early stopping occured at epoch 186 with best_epoch = 166 and best_val_logits_ll = 0.0193\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD4_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD4_.pth.zip\n",
      "Seed:4,Fold:5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34447 | val_logits_ll: 0.03516 |  0:00:02s\n",
      "epoch 10 | loss: 0.02161 | val_logits_ll: 0.02164 |  0:00:27s\n",
      "epoch 20 | loss: 0.02035 | val_logits_ll: 0.02089 |  0:00:54s\n",
      "epoch 30 | loss: 0.02002 | val_logits_ll: 0.01989 |  0:01:19s\n",
      "epoch 40 | loss: 0.01985 | val_logits_ll: 0.01957 |  0:01:45s\n",
      "epoch 50 | loss: 0.01981 | val_logits_ll: 0.01957 |  0:02:11s\n",
      "epoch 60 | loss: 0.01976 | val_logits_ll: 0.01955 |  0:02:37s\n",
      "epoch 70 | loss: 0.01966 | val_logits_ll: 0.01931 |  0:03:02s\n",
      "epoch 80 | loss: 0.0197  | val_logits_ll: 0.01942 |  0:03:29s\n",
      "epoch 90 | loss: 0.01966 | val_logits_ll: 0.01933 |  0:03:55s\n",
      "epoch 100| loss: 0.01966 | val_logits_ll: 0.01926 |  0:04:20s\n",
      "epoch 110| loss: 0.01962 | val_logits_ll: 0.01928 |  0:04:46s\n",
      "epoch 120| loss: 0.01963 | val_logits_ll: 0.01929 |  0:05:12s\n",
      "epoch 130| loss: 0.01961 | val_logits_ll: 0.01927 |  0:05:37s\n",
      "epoch 140| loss: 0.01958 | val_logits_ll: 0.01924 |  0:06:03s\n",
      "epoch 150| loss: 0.01956 | val_logits_ll: 0.01925 |  0:06:29s\n",
      "epoch 160| loss: 0.01954 | val_logits_ll: 0.0192  |  0:06:55s\n",
      "epoch 170| loss: 0.01951 | val_logits_ll: 0.01924 |  0:07:21s\n",
      "epoch 180| loss: 0.01952 | val_logits_ll: 0.01922 |  0:07:47s\n",
      "\n",
      "Early stopping occured at epoch 180 with best_epoch = 160 and best_val_logits_ll = 0.0192\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD5_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD5_.pth.zip\n",
      "Seed:4,Fold:6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.34842 | val_logits_ll: 0.0365  |  0:00:02s\n",
      "epoch 10 | loss: 0.02152 | val_logits_ll: 0.02128 |  0:00:28s\n",
      "epoch 20 | loss: 0.02016 | val_logits_ll: 0.02204 |  0:00:53s\n",
      "epoch 30 | loss: 0.01995 | val_logits_ll: 0.02    |  0:01:19s\n",
      "epoch 40 | loss: 0.01987 | val_logits_ll: 0.0203  |  0:01:45s\n",
      "epoch 50 | loss: 0.0198  | val_logits_ll: 0.0196  |  0:02:10s\n",
      "epoch 60 | loss: 0.01977 | val_logits_ll: 0.01963 |  0:02:36s\n",
      "epoch 70 | loss: 0.01964 | val_logits_ll: 0.01949 |  0:03:01s\n",
      "epoch 80 | loss: 0.01964 | val_logits_ll: 0.01953 |  0:03:27s\n",
      "epoch 90 | loss: 0.01962 | val_logits_ll: 0.01952 |  0:03:54s\n",
      "epoch 100| loss: 0.01963 | val_logits_ll: 0.01951 |  0:04:20s\n",
      "\n",
      "Early stopping occured at epoch 104 with best_epoch = 84 and best_val_logits_ll = 0.01946\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD6_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD6_.pth.zip\n",
      "Seed:5,Fold:0\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36965 | val_logits_ll: 0.03763 |  0:00:02s\n",
      "epoch 10 | loss: 0.02101 | val_logits_ll: 0.02188 |  0:00:28s\n",
      "epoch 20 | loss: 0.02023 | val_logits_ll: 0.0232  |  0:00:54s\n",
      "epoch 30 | loss: 0.01995 | val_logits_ll: 0.02001 |  0:01:20s\n",
      "epoch 40 | loss: 0.01989 | val_logits_ll: 0.01967 |  0:01:46s\n",
      "epoch 50 | loss: 0.01976 | val_logits_ll: 0.01963 |  0:02:11s\n",
      "epoch 60 | loss: 0.01975 | val_logits_ll: 0.01956 |  0:02:37s\n",
      "epoch 70 | loss: 0.01969 | val_logits_ll: 0.01952 |  0:03:02s\n",
      "epoch 80 | loss: 0.01965 | val_logits_ll: 0.0195  |  0:03:27s\n",
      "epoch 90 | loss: 0.01968 | val_logits_ll: 0.01963 |  0:03:53s\n",
      "epoch 100| loss: 0.01961 | val_logits_ll: 0.01945 |  0:04:19s\n",
      "epoch 110| loss: 0.01958 | val_logits_ll: 0.01945 |  0:04:45s\n",
      "epoch 120| loss: 0.01956 | val_logits_ll: 0.01938 |  0:05:11s\n",
      "epoch 130| loss: 0.01956 | val_logits_ll: 0.01941 |  0:05:36s\n",
      "epoch 140| loss: 0.01955 | val_logits_ll: 0.01939 |  0:06:02s\n",
      "epoch 150| loss: 0.01952 | val_logits_ll: 0.01937 |  0:06:27s\n",
      "\n",
      "Early stopping occured at epoch 153 with best_epoch = 133 and best_val_logits_ll = 0.01936\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD0_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD0_.pth.zip\n",
      "Seed:5,Fold:1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36845 | val_logits_ll: 0.03726 |  0:00:02s\n",
      "epoch 10 | loss: 0.0213  | val_logits_ll: 0.02103 |  0:00:28s\n",
      "epoch 20 | loss: 0.02028 | val_logits_ll: 0.02159 |  0:00:53s\n",
      "epoch 30 | loss: 0.01995 | val_logits_ll: 0.01999 |  0:01:19s\n",
      "epoch 40 | loss: 0.01991 | val_logits_ll: 0.01976 |  0:01:44s\n",
      "epoch 50 | loss: 0.01977 | val_logits_ll: 0.0195  |  0:02:10s\n",
      "epoch 60 | loss: 0.0197  | val_logits_ll: 0.01937 |  0:02:36s\n",
      "epoch 70 | loss: 0.01969 | val_logits_ll: 0.01932 |  0:03:01s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01939 |  0:03:27s\n",
      "epoch 90 | loss: 0.01962 | val_logits_ll: 0.01932 |  0:03:53s\n",
      "epoch 100| loss: 0.01963 | val_logits_ll: 0.01924 |  0:04:17s\n",
      "epoch 110| loss: 0.01956 | val_logits_ll: 0.01923 |  0:04:43s\n",
      "epoch 120| loss: 0.01959 | val_logits_ll: 0.01928 |  0:05:09s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.01922 |  0:05:34s\n",
      "\n",
      "Early stopping occured at epoch 136 with best_epoch = 116 and best_val_logits_ll = 0.01921\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD1_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD1_.pth.zip\n",
      "Seed:5,Fold:2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.3638  | val_logits_ll: 0.03757 |  0:00:02s\n",
      "epoch 10 | loss: 0.02105 | val_logits_ll: 0.02158 |  0:00:28s\n",
      "epoch 20 | loss: 0.02017 | val_logits_ll: 0.02209 |  0:00:53s\n",
      "epoch 30 | loss: 0.01992 | val_logits_ll: 0.01979 |  0:01:19s\n",
      "epoch 40 | loss: 0.01987 | val_logits_ll: 0.02037 |  0:01:45s\n",
      "epoch 50 | loss: 0.01976 | val_logits_ll: 0.0196  |  0:02:11s\n",
      "epoch 60 | loss: 0.01972 | val_logits_ll: 0.01959 |  0:02:36s\n",
      "epoch 70 | loss: 0.01977 | val_logits_ll: 0.01958 |  0:03:02s\n",
      "epoch 80 | loss: 0.01965 | val_logits_ll: 0.01951 |  0:03:27s\n",
      "epoch 90 | loss: 0.01962 | val_logits_ll: 0.01947 |  0:03:53s\n",
      "epoch 100| loss: 0.01958 | val_logits_ll: 0.01944 |  0:04:20s\n",
      "epoch 110| loss: 0.01959 | val_logits_ll: 0.01949 |  0:04:45s\n",
      "epoch 120| loss: 0.01956 | val_logits_ll: 0.01942 |  0:05:11s\n",
      "epoch 130| loss: 0.01955 | val_logits_ll: 0.01942 |  0:05:36s\n",
      "epoch 140| loss: 0.01956 | val_logits_ll: 0.01943 |  0:06:02s\n",
      "epoch 150| loss: 0.01952 | val_logits_ll: 0.01941 |  0:06:28s\n",
      "epoch 160| loss: 0.01953 | val_logits_ll: 0.01944 |  0:06:53s\n",
      "epoch 170| loss: 0.01952 | val_logits_ll: 0.01937 |  0:07:18s\n",
      "epoch 180| loss: 0.01951 | val_logits_ll: 0.01935 |  0:07:45s\n",
      "epoch 190| loss: 0.01948 | val_logits_ll: 0.01938 |  0:08:10s\n",
      "\n",
      "Early stopping occured at epoch 196 with best_epoch = 176 and best_val_logits_ll = 0.01935\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD2_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD2_.pth.zip\n",
      "Seed:5,Fold:3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36353 | val_logits_ll: 0.03485 |  0:00:02s\n",
      "epoch 10 | loss: 0.02108 | val_logits_ll: 0.02251 |  0:00:27s\n",
      "epoch 20 | loss: 0.02016 | val_logits_ll: 0.01987 |  0:00:51s\n",
      "epoch 30 | loss: 0.01996 | val_logits_ll: 0.01972 |  0:01:16s\n",
      "epoch 40 | loss: 0.01975 | val_logits_ll: 0.01943 |  0:01:43s\n",
      "epoch 50 | loss: 0.01998 | val_logits_ll: 0.01971 |  0:02:09s\n",
      "epoch 60 | loss: 0.01981 | val_logits_ll: 0.0194  |  0:02:34s\n",
      "epoch 70 | loss: 0.01975 | val_logits_ll: 0.01947 |  0:02:57s\n",
      "epoch 80 | loss: 0.01962 | val_logits_ll: 0.01927 |  0:03:23s\n",
      "epoch 90 | loss: 0.01959 | val_logits_ll: 0.01931 |  0:03:50s\n",
      "epoch 100| loss: 0.01962 | val_logits_ll: 0.01929 |  0:04:15s\n",
      "epoch 110| loss: 0.01959 | val_logits_ll: 0.01925 |  0:04:39s\n",
      "epoch 120| loss: 0.01955 | val_logits_ll: 0.01923 |  0:05:05s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.01922 |  0:05:30s\n",
      "\n",
      "Early stopping occured at epoch 132 with best_epoch = 112 and best_val_logits_ll = 0.01921\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD3_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD3_.pth.zip\n",
      "Seed:5,Fold:4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.36549 | val_logits_ll: 0.03632 |  0:00:02s\n",
      "epoch 10 | loss: 0.0212  | val_logits_ll: 0.02071 |  0:00:28s\n",
      "epoch 20 | loss: 0.02018 | val_logits_ll: 0.01981 |  0:00:54s\n",
      "epoch 30 | loss: 0.01989 | val_logits_ll: 0.01972 |  0:01:19s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40 | loss: 0.01987 | val_logits_ll: 0.01976 |  0:01:45s\n",
      "epoch 50 | loss: 0.01969 | val_logits_ll: 0.01944 |  0:02:11s\n",
      "epoch 60 | loss: 0.01974 | val_logits_ll: 0.01942 |  0:02:36s\n",
      "epoch 70 | loss: 0.01964 | val_logits_ll: 0.01936 |  0:03:02s\n",
      "epoch 80 | loss: 0.0196  | val_logits_ll: 0.01936 |  0:03:28s\n",
      "epoch 90 | loss: 0.0196  | val_logits_ll: 0.01938 |  0:03:54s\n",
      "epoch 100| loss: 0.01957 | val_logits_ll: 0.01931 |  0:04:20s\n",
      "epoch 110| loss: 0.01956 | val_logits_ll: 0.0193  |  0:04:45s\n",
      "epoch 120| loss: 0.01952 | val_logits_ll: 0.01928 |  0:05:12s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.01929 |  0:05:39s\n",
      "epoch 140| loss: 0.01953 | val_logits_ll: 0.0193  |  0:06:04s\n",
      "epoch 150| loss: 0.01948 | val_logits_ll: 0.01929 |  0:06:31s\n",
      "epoch 160| loss: 0.0195  | val_logits_ll: 0.01928 |  0:06:56s\n",
      "epoch 170| loss: 0.0195  | val_logits_ll: 0.01926 |  0:07:22s\n",
      "epoch 180| loss: 0.01948 | val_logits_ll: 0.01924 |  0:07:46s\n",
      "epoch 190| loss: 0.01948 | val_logits_ll: 0.01926 |  0:08:12s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 198 and best_val_logits_ll = 0.01924\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD4_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD4_.pth.zip\n",
      "Seed:5,Fold:5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37332 | val_logits_ll: 0.04061 |  0:00:02s\n",
      "epoch 10 | loss: 0.02156 | val_logits_ll: 0.02104 |  0:00:26s\n",
      "epoch 20 | loss: 0.02021 | val_logits_ll: 0.02039 |  0:00:51s\n",
      "epoch 30 | loss: 0.01996 | val_logits_ll: 0.01975 |  0:01:16s\n",
      "epoch 40 | loss: 0.01977 | val_logits_ll: 0.01969 |  0:01:42s\n",
      "epoch 50 | loss: 0.01971 | val_logits_ll: 0.01959 |  0:02:08s\n",
      "epoch 60 | loss: 0.01981 | val_logits_ll: 0.01979 |  0:02:34s\n",
      "epoch 70 | loss: 0.01966 | val_logits_ll: 0.01991 |  0:03:00s\n",
      "epoch 80 | loss: 0.01986 | val_logits_ll: 0.01971 |  0:03:26s\n",
      "epoch 90 | loss: 0.01962 | val_logits_ll: 0.01947 |  0:03:53s\n",
      "epoch 100| loss: 0.01957 | val_logits_ll: 0.01947 |  0:04:19s\n",
      "epoch 110| loss: 0.01954 | val_logits_ll: 0.01945 |  0:04:45s\n",
      "\n",
      "Early stopping occured at epoch 112 with best_epoch = 92 and best_val_logits_ll = 0.01943\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD5_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD5_.pth.zip\n",
      "Seed:5,Fold:6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37145 | val_logits_ll: 0.03961 |  0:00:02s\n",
      "epoch 10 | loss: 0.02102 | val_logits_ll: 0.02083 |  0:00:27s\n",
      "epoch 20 | loss: 0.02015 | val_logits_ll: 0.02025 |  0:00:53s\n",
      "epoch 30 | loss: 0.01994 | val_logits_ll: 0.01981 |  0:01:17s\n",
      "epoch 40 | loss: 0.01984 | val_logits_ll: 0.01965 |  0:01:42s\n",
      "epoch 50 | loss: 0.01977 | val_logits_ll: 0.01963 |  0:02:07s\n",
      "epoch 60 | loss: 0.01981 | val_logits_ll: 0.01965 |  0:02:32s\n",
      "epoch 70 | loss: 0.01972 | val_logits_ll: 0.01952 |  0:02:58s\n",
      "epoch 80 | loss: 0.01963 | val_logits_ll: 0.01957 |  0:03:23s\n",
      "epoch 90 | loss: 0.0196  | val_logits_ll: 0.01946 |  0:03:49s\n",
      "epoch 100| loss: 0.01962 | val_logits_ll: 0.01947 |  0:04:15s\n",
      "epoch 110| loss: 0.01957 | val_logits_ll: 0.01941 |  0:04:40s\n",
      "epoch 120| loss: 0.01955 | val_logits_ll: 0.01943 |  0:05:06s\n",
      "epoch 130| loss: 0.01955 | val_logits_ll: 0.01944 |  0:05:32s\n",
      "epoch 140| loss: 0.01954 | val_logits_ll: 0.01944 |  0:05:57s\n",
      "epoch 150| loss: 0.01956 | val_logits_ll: 0.0194  |  0:06:24s\n",
      "epoch 160| loss: 0.01954 | val_logits_ll: 0.0194  |  0:06:50s\n",
      "epoch 170| loss: 0.01948 | val_logits_ll: 0.01941 |  0:07:15s\n",
      "epoch 180| loss: 0.01948 | val_logits_ll: 0.01937 |  0:07:41s\n",
      "epoch 190| loss: 0.01948 | val_logits_ll: 0.01937 |  0:08:05s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 193 and best_val_logits_ll = 0.01934\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD6_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD6_.pth.zip\n",
      "Seed:6,Fold:0\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37309 | val_logits_ll: 0.03912 |  0:00:02s\n",
      "epoch 10 | loss: 0.0217  | val_logits_ll: 0.02216 |  0:00:27s\n",
      "epoch 20 | loss: 0.02021 | val_logits_ll: 0.02045 |  0:00:53s\n",
      "epoch 30 | loss: 0.02001 | val_logits_ll: 0.02037 |  0:01:18s\n",
      "epoch 40 | loss: 0.01986 | val_logits_ll: 0.01976 |  0:01:43s\n",
      "epoch 50 | loss: 0.01978 | val_logits_ll: 0.02059 |  0:02:08s\n",
      "epoch 60 | loss: 0.01975 | val_logits_ll: 0.01954 |  0:02:33s\n",
      "epoch 70 | loss: 0.01967 | val_logits_ll: 0.01946 |  0:02:59s\n",
      "epoch 80 | loss: 0.01963 | val_logits_ll: 0.01942 |  0:03:25s\n",
      "epoch 90 | loss: 0.01969 | val_logits_ll: 0.01943 |  0:03:52s\n",
      "epoch 100| loss: 0.01962 | val_logits_ll: 0.01939 |  0:04:19s\n",
      "epoch 110| loss: 0.01956 | val_logits_ll: 0.01936 |  0:04:44s\n",
      "epoch 120| loss: 0.01962 | val_logits_ll: 0.01939 |  0:05:10s\n",
      "epoch 130| loss: 0.01957 | val_logits_ll: 0.01939 |  0:05:37s\n",
      "epoch 140| loss: 0.01955 | val_logits_ll: 0.01938 |  0:06:03s\n",
      "\n",
      "Early stopping occured at epoch 148 with best_epoch = 128 and best_val_logits_ll = 0.01932\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD0_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD0_.pth.zip\n",
      "Seed:6,Fold:1\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37427 | val_logits_ll: 0.03754 |  0:00:02s\n",
      "epoch 10 | loss: 0.02141 | val_logits_ll: 0.0221  |  0:00:24s\n",
      "epoch 20 | loss: 0.02029 | val_logits_ll: 0.02233 |  0:00:48s\n",
      "epoch 30 | loss: 0.01998 | val_logits_ll: 0.02178 |  0:01:14s\n",
      "epoch 40 | loss: 0.01986 | val_logits_ll: 0.01988 |  0:01:41s\n",
      "epoch 50 | loss: 0.0198  | val_logits_ll: 0.01963 |  0:02:07s\n",
      "epoch 60 | loss: 0.01995 | val_logits_ll: 0.02046 |  0:02:32s\n",
      "epoch 70 | loss: 0.01969 | val_logits_ll: 0.01953 |  0:02:59s\n",
      "epoch 80 | loss: 0.01974 | val_logits_ll: 0.01946 |  0:03:24s\n",
      "epoch 90 | loss: 0.01965 | val_logits_ll: 0.01938 |  0:03:50s\n",
      "epoch 100| loss: 0.01963 | val_logits_ll: 0.01945 |  0:04:15s\n",
      "epoch 110| loss: 0.01963 | val_logits_ll: 0.01939 |  0:04:40s\n",
      "epoch 120| loss: 0.01959 | val_logits_ll: 0.01935 |  0:05:04s\n",
      "epoch 130| loss: 0.01957 | val_logits_ll: 0.01934 |  0:05:29s\n",
      "epoch 140| loss: 0.01957 | val_logits_ll: 0.01932 |  0:05:54s\n",
      "epoch 150| loss: 0.01958 | val_logits_ll: 0.01937 |  0:06:19s\n",
      "epoch 160| loss: 0.01955 | val_logits_ll: 0.0193  |  0:06:44s\n",
      "epoch 170| loss: 0.01958 | val_logits_ll: 0.01932 |  0:07:10s\n",
      "epoch 180| loss: 0.01953 | val_logits_ll: 0.01931 |  0:07:35s\n",
      "epoch 190| loss: 0.01951 | val_logits_ll: 0.01928 |  0:08:02s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 189 and best_val_logits_ll = 0.01927\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD1_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD1_.pth.zip\n",
      "Seed:6,Fold:2\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37479 | val_logits_ll: 0.04111 |  0:00:02s\n",
      "epoch 10 | loss: 0.02134 | val_logits_ll: 0.02172 |  0:00:28s\n",
      "epoch 20 | loss: 0.02022 | val_logits_ll: 0.02156 |  0:00:54s\n",
      "epoch 30 | loss: 0.02004 | val_logits_ll: 0.01992 |  0:01:19s\n",
      "epoch 40 | loss: 0.01987 | val_logits_ll: 0.01973 |  0:01:44s\n",
      "epoch 50 | loss: 0.01982 | val_logits_ll: 0.01958 |  0:02:09s\n",
      "epoch 60 | loss: 0.01977 | val_logits_ll: 0.01951 |  0:02:36s\n",
      "epoch 70 | loss: 0.01971 | val_logits_ll: 0.01944 |  0:03:01s\n",
      "epoch 80 | loss: 0.0197  | val_logits_ll: 0.01941 |  0:03:27s\n",
      "epoch 90 | loss: 0.0197  | val_logits_ll: 0.01952 |  0:03:54s\n",
      "epoch 100| loss: 0.01958 | val_logits_ll: 0.01932 |  0:04:20s\n",
      "epoch 110| loss: 0.01962 | val_logits_ll: 0.01935 |  0:04:45s\n",
      "epoch 120| loss: 0.0196  | val_logits_ll: 0.01933 |  0:05:11s\n",
      "epoch 130| loss: 0.01956 | val_logits_ll: 0.01932 |  0:05:37s\n",
      "epoch 140| loss: 0.01958 | val_logits_ll: 0.01933 |  0:06:02s\n",
      "epoch 150| loss: 0.01954 | val_logits_ll: 0.0193  |  0:06:28s\n",
      "epoch 160| loss: 0.01953 | val_logits_ll: 0.01938 |  0:06:54s\n",
      "epoch 170| loss: 0.01952 | val_logits_ll: 0.01928 |  0:07:19s\n",
      "epoch 180| loss: 0.01953 | val_logits_ll: 0.01931 |  0:07:44s\n",
      "epoch 190| loss: 0.01954 | val_logits_ll: 0.01932 |  0:08:09s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 184 and best_val_logits_ll = 0.01925\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD2_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD2_.pth.zip\n",
      "Seed:6,Fold:3\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37885 | val_logits_ll: 0.04227 |  0:00:02s\n",
      "epoch 10 | loss: 0.0215  | val_logits_ll: 0.02234 |  0:00:29s\n",
      "epoch 20 | loss: 0.02029 | val_logits_ll: 0.02099 |  0:00:56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.01997 | val_logits_ll: 0.01974 |  0:01:21s\n",
      "epoch 40 | loss: 0.01983 | val_logits_ll: 0.01939 |  0:01:46s\n",
      "epoch 50 | loss: 0.01979 | val_logits_ll: 0.01939 |  0:02:11s\n",
      "epoch 60 | loss: 0.01981 | val_logits_ll: 0.01952 |  0:02:35s\n",
      "epoch 70 | loss: 0.01975 | val_logits_ll: 0.01945 |  0:03:00s\n",
      "epoch 80 | loss: 0.01962 | val_logits_ll: 0.01924 |  0:03:26s\n",
      "epoch 90 | loss: 0.01961 | val_logits_ll: 0.01929 |  0:03:52s\n",
      "epoch 100| loss: 0.01962 | val_logits_ll: 0.01924 |  0:04:18s\n",
      "\n",
      "Early stopping occured at epoch 100 with best_epoch = 80 and best_val_logits_ll = 0.01924\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD3_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD3_.pth.zip\n",
      "Seed:6,Fold:4\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37073 | val_logits_ll: 0.04248 |  0:00:02s\n",
      "epoch 10 | loss: 0.02165 | val_logits_ll: 0.02253 |  0:00:27s\n",
      "epoch 20 | loss: 0.02023 | val_logits_ll: 0.02222 |  0:00:53s\n",
      "epoch 30 | loss: 0.02003 | val_logits_ll: 0.02078 |  0:01:19s\n",
      "epoch 40 | loss: 0.01987 | val_logits_ll: 0.01977 |  0:01:44s\n",
      "epoch 50 | loss: 0.01976 | val_logits_ll: 0.01973 |  0:02:10s\n",
      "epoch 60 | loss: 0.0197  | val_logits_ll: 0.01967 |  0:02:37s\n",
      "epoch 70 | loss: 0.01968 | val_logits_ll: 0.0196  |  0:03:03s\n",
      "epoch 80 | loss: 0.01963 | val_logits_ll: 0.01951 |  0:03:29s\n",
      "epoch 90 | loss: 0.0196  | val_logits_ll: 0.01949 |  0:03:55s\n",
      "epoch 100| loss: 0.01957 | val_logits_ll: 0.01952 |  0:04:22s\n",
      "epoch 110| loss: 0.01955 | val_logits_ll: 0.01951 |  0:04:47s\n",
      "epoch 120| loss: 0.01962 | val_logits_ll: 0.01948 |  0:05:12s\n",
      "epoch 130| loss: 0.01953 | val_logits_ll: 0.01949 |  0:05:37s\n",
      "epoch 140| loss: 0.01954 | val_logits_ll: 0.01948 |  0:06:03s\n",
      "epoch 150| loss: 0.01949 | val_logits_ll: 0.01944 |  0:06:28s\n",
      "\n",
      "Early stopping occured at epoch 153 with best_epoch = 133 and best_val_logits_ll = 0.01943\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD4_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD4_.pth.zip\n",
      "Seed:6,Fold:5\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37374 | val_logits_ll: 0.04069 |  0:00:02s\n",
      "epoch 10 | loss: 0.02118 | val_logits_ll: 0.02215 |  0:00:27s\n",
      "epoch 20 | loss: 0.02021 | val_logits_ll: 0.02096 |  0:00:52s\n",
      "epoch 30 | loss: 0.01996 | val_logits_ll: 0.02008 |  0:01:19s\n",
      "epoch 40 | loss: 0.0199  | val_logits_ll: 0.02005 |  0:01:45s\n",
      "epoch 50 | loss: 0.01989 | val_logits_ll: 0.01962 |  0:02:11s\n",
      "epoch 60 | loss: 0.01973 | val_logits_ll: 0.01959 |  0:02:37s\n",
      "epoch 70 | loss: 0.01967 | val_logits_ll: 0.01961 |  0:03:03s\n",
      "epoch 80 | loss: 0.01962 | val_logits_ll: 0.01948 |  0:03:29s\n",
      "epoch 90 | loss: 0.0197  | val_logits_ll: 0.01953 |  0:03:55s\n",
      "epoch 100| loss: 0.01957 | val_logits_ll: 0.01945 |  0:04:21s\n",
      "epoch 110| loss: 0.01966 | val_logits_ll: 0.01948 |  0:04:47s\n",
      "epoch 120| loss: 0.01957 | val_logits_ll: 0.01941 |  0:05:12s\n",
      "epoch 130| loss: 0.01954 | val_logits_ll: 0.01939 |  0:05:37s\n",
      "epoch 140| loss: 0.01953 | val_logits_ll: 0.0194  |  0:06:03s\n",
      "epoch 150| loss: 0.01954 | val_logits_ll: 0.01943 |  0:06:29s\n",
      "\n",
      "Early stopping occured at epoch 159 with best_epoch = 139 and best_val_logits_ll = 0.01936\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD5_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD5_.pth.zip\n",
      "Seed:6,Fold:6\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.37779 | val_logits_ll: 0.04193 |  0:00:02s\n",
      "epoch 10 | loss: 0.02116 | val_logits_ll: 0.02076 |  0:00:28s\n",
      "epoch 20 | loss: 0.02015 | val_logits_ll: 0.02003 |  0:00:55s\n",
      "epoch 30 | loss: 0.01997 | val_logits_ll: 0.01981 |  0:01:19s\n",
      "epoch 40 | loss: 0.01988 | val_logits_ll: 0.01959 |  0:01:44s\n",
      "epoch 50 | loss: 0.01973 | val_logits_ll: 0.01952 |  0:02:10s\n",
      "epoch 60 | loss: 0.01982 | val_logits_ll: 0.01951 |  0:02:36s\n",
      "epoch 70 | loss: 0.0197  | val_logits_ll: 0.01944 |  0:03:02s\n",
      "epoch 80 | loss: 0.01966 | val_logits_ll: 0.01948 |  0:03:26s\n",
      "epoch 90 | loss: 0.01961 | val_logits_ll: 0.01937 |  0:03:52s\n",
      "epoch 100| loss: 0.01963 | val_logits_ll: 0.01939 |  0:04:18s\n",
      "epoch 110| loss: 0.01961 | val_logits_ll: 0.01952 |  0:04:43s\n",
      "\n",
      "Early stopping occured at epoch 110 with best_epoch = 90 and best_val_logits_ll = 0.01937\n",
      "Best weights from best epoch are automatically used!\n",
      "Model Saving: tabnet_v2_FOLD6_.pth\n",
      "Successfully saved model at tabnet_v2_FOLD6_.pth.zip\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "SEED = [0, 1, 2, 3, 4, 5, 6]\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "time_begin = time()\n",
    "\n",
    "for seed_id in SEED:\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "time_diff = time() - time_begin\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3016611176066926"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_diff/60/60,0.015603591991583895,16,96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.015603591991583895\n"
     ]
    }
   ],
   "source": [
    "valid_results = scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "y_true = scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "\n",
    "for i in range(len(target_cols)):\n",
    "    score += log_loss(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "print(\"CV log_loss: \", score / y_pred.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 207)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "sub1.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV log_loss:  0.01561899863439966,  pb:0.01834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
